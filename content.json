{"pages":[{"title":"","text":"","link":"/books/index.html"},{"title":"tingshushaw的介绍","text":"(ง •_•)ง 联系方式: QQ:443762712 邮箱：443762712@qq.com（欢迎各位浏览个人博客，并提出问题与建议） 普通一本院校，数据科学与大数据技术专业，具备数据分析与数据挖掘的能力，能运用python以及Hadoop等大数据相关技术解决问题。目前对问题的思考与探索能力有待提升。 准备考研，其方向为地图制图学与地理信息系统，在此希望2020年12月份的研究生考试能够成功上岸 热爱生活，但有点懒惰 愿望：希望世界和平☮","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"Daily Reading","text":"","link":"/Daily-Reading/index.html"}],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/03/09/hello-world/"},{"title":"Flume分布式日志框架","text":"Flume的简单介绍、架构、OG、NG、核心组件官网 ： http://flume.apache.org/ 为什么要使用Flume主要作用：海量日志采集、聚合和传输的系统 支持在日志系统中制定各类数据发送方，用户收集数据；提供对数据进行简单处理，可写到各种数据接收方(文本、HDFS、HBase) Flume可靠性 End-to-end:先写在磁盘上，当数据传送成功后，在删除；如果数据发送失败，可以重新发送 Store on failure : scribe采用的策略，当数据放crash时，将数据写到本地，待恢复后，继续发送 Best effort：数据发送到接收后，不会进行确认 Flume架构版本：Flume OG 0.9x , Flume NG 1.x agent中过程 1 source —&gt; n channel 1 channel —&gt; 1sink 1 sink —-&gt; channel 核心组件介绍 flume.source flume.channel flume.sink Flume配置文件的编写配置文件规则：source_channel_sink配置文件名内容：123456789101112#定义各个组件agent1.sources = srcagent1.channels = chagent1.sinks = des#配置source,channel,sinkagent1.sources.src.type = execagent1.sources.src.command = tail -F /data/mydata/sample_dataagent1.channels.ch.type = memoryagent1.sinks.des.tyoe = logger#关联(用点把线连起来)agent1.sources.src.channels = chagent1.sinks.des.channel = ch 配置文件的调用：123456789flume-ng agent \\-c /data/script \\-f /data/script/exec_mem_logger.conf \\-n agent1 \\-D flume.root.logger=DEBUG,console#-c 配置文件存放的目录#-f 所使用的配置文件路径#-n agent的名称","link":"/2020/04/05/BigData/Flume%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/"},{"title":"HBase shell操作","text":"进入HBase命令行HBase提供了一个shell的终端给用户进行交互，可以与MySQL关联学习 1234#前提启动Hadoop与hbase服务#进入hbase命令行$ cd HBASE_HOME/bin/$ hbase shell HBase中基本操作 名称 命令表达式 创建表 create ‘table_name’,’col_family1’,’col_family2’,,,,,’col_familyN’, 删除记录 put “table_name”,”row_name”,”col_name”,”value” 查看记录 get “table_name”,”col_name” 查看表中记录总数 count “table_name” 删除记录 delete “table_name”,”row_name”,”col_name” 删除一张表 先要屏蔽该表，才能对该表进行删除，第一步 disable “table_name” 第二部 drop “table_name” 查看所有记录 scan “table_name” 查看某一个表某一列的所有数据 scan “table_name”,{COLUMNS=&gt; ‘col_family: col_name’} 更新记录 重写一遍进行覆盖 表操作 创建表：&gt; create 'table_name','col_family1','col_family2',,,,,'col_familyN' 列出全部表：&gt; list 得到表的描述：&gt; describe &quot;table_name&quot; 删除表：&gt; disable &quot;table_name&quot;;drop &quot;table_name&quot; 判断表是否启用：is enable &quot;table_name&quot; 数据操作 添加记录：&gt; put 获取一条记录： 取得一个id的所有数据: \"tables\",\"row_key\"```12342. 获取一个id（row_key），一个列族的所有数据: ```get &quot;table_name&quot;,&quot;row_key&quot;,&quot;info&quot; 获取一个id，一个列族中列的所有数据： \"table_name\",\"row_key\",\"info:col\"```12345678- 更新记录 ```shell #更新users表中小明的年龄 &gt; put &quot;users&quot;,&quot;xiaoming&quot;,&quot;info:age&quot;,&quot;29&quot; #查看小明的年龄 &gt; get &quot;users&quot;,&quot;xiaoming&quot;,&quot;info:age&quot; 获取单元格数据的版本数据 1234&gt; get \"users\",\"xiaoming\",{COLUMN=&gt;'info:age',VERSION=&gt;1}&gt; get \"users\",\"xiaoming\",{COLUMN=&gt;'info:age',VERSION=&gt;2}&gt; get \"users\",\"xiaoming\",{COLUMN=&gt;'info:age',VERSION=&gt;3}[out-put] :只有一个版本数据，如果想看见多个版本，需要在配置文件中进行相关配置 获取单元格数据的某个版本数据(timestamp) 1&gt; get \"users\",\"xiaoming\",{COLUMN=&gt;'info:age',TIMESTAMP=&gt;1442910320009} 删除操作 12345678#删除小明的age字段&gt; delete \"users\",\"xiaoming\",\"info:age\"# 删除整行，把小明全部删掉&gt; deleteall \"users\",\"xioaming\"#统计表的行数&gt; count \"users\"#清空表 **慎用**&gt; truncate \"users\"","link":"/2020/04/07/BigData/HBase-shell%E6%93%8D%E4%BD%9C/"},{"title":"HBase Java API","text":"主函数与创建一个表图1","link":"/2020/04/08/BigData/HBase-Java-API/"},{"title":"HBase概述","text":"HBase介绍HBase - Hadoop Database,是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可以在廉价的PC SERVER上搭建大规模结构化数据存储的集群。 HBase利用Hadoop HDFS 作为其文件存储系统， 利用Hadoop Mapreduce来处理HBase中的海量数据，利用Zookeeper作为协调工具。 可以单独运行，也可以与mapreduce关联使用，且必须依赖zookeeper 什么情况下使用HBase(一定程度上弥补了HDFS缺陷) 需要对数据进行随机读操作或者随机写操作 大数据量高并发操作，比如每秒对PB级数据进行上千次操作 读写访问均是非常简单操作 逻辑数据模型 行键具有唯一性； 列没有必要提前写好 没有写数据的区域不会占据物理空间 行键： 字符串、整数、二进制串甚至串行化的结构都可以作为行键 表按照行键的“逐字节排序”顺序对行进行有序化处理 列族： 表中至少有一个列族 “族：标签”其中，族和标签都可为任意形式的串 物理上将通“族”数据存储在一起 物理数据模型 数据存储 系统架构 角色 作用 client 包含访问hbase的接口，client维护者一些cache来加快对hbase的访问，比如region的位置信息 zookeeper 保证任何时候，集群中只有一个runningmaster；存贮所有region的寻址入口；实时监控region server的状态，将其上下线信息实时通知给master；存储hbase的schema，包括元数据包括有哪些table master 可以启动多个master，通过zookeeper的master election机制保证总有一个master运行；为regionserver分配region；负责region server的负载均衡；发现失效的region server 并重新分配其上的region region server 维护master分配给它region，处理region的IO请求；负责切分运行过程中变得过大的region region定位 ROOT表存储了META表所在region的位置，root表只有一个region； zookeeper file记录了 -ROOT表的region位置； META表存储所有用户表的region的位置； Table在行的方向上分割为多个HRegion,一个region由[startkey,eendkey)表示，每个HRegion分散在不同的regionserver中； HBase访问接口 Native Java API HBase shell Thrift Gateway REST Gateway Pig&amp; Hive","link":"/2020/04/07/BigData/HBase%E6%A6%82%E8%BF%B0/"},{"title":"Hadoopop基础学习","text":"","link":"/2020/03/26/BigData/Hadoopop%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"title":"Hadoop——HDFS学习","text":"HDFS理论讲解HDFS理论讲解分布式结构集群和分布式概念集群：指逻辑上处理同一任务的机器集合，可以属于同意机房，也可分属不同机房。 分布式：分布式文件系统把文件分布存储到多个计算机节点上，成千上万的计算机节点构成计算机集群。 计算机集群基本架构： 。。。。。。。图。。。。。。。 目前的分布式文件系统所采用的计算机集群，都是由普通硬件构成，大大降低了硬件上的开销 分布式文件系统的结构HDFS架构和相关概念HDFS数据操作HDFS数据处理原理HDFS基本操作HDFS目录文件HDFS文件操作HDFS查看文件信息HDFS压缩与解压缩HDFS数据读写过程HDFS数据读写过程","link":"/2020/03/26/BigData/Hadoop%E2%80%94%E2%80%94HDFS%E5%AD%A6%E4%B9%A0/"},{"title":"Kafka分布式的消息队列","text":"Kafka的基本介绍什么是Kafka 分布式、可分区、可复制的消息系统 LinkedIn开发，Scala编写 支持水平扩展和高吞吐率 可与Apache Storm、Spark等多种开源分布式处理系统集成 kafak官网 kafak学习手册 应用场景 构建应用系统和分析系统的桥梁，并将他们之间的关联解耦(将两者关联并保持关联有一定距离) 支持近实时的在线分析系统和类似于Hadoop之类的离线分析系统 具有高可扩展性。即当数据量增加时，可以通过增加节点进行水平扩展 Kafka 消息队列 VS Flume 数据采集 source sink组件123kafka project process: data collection --&gt; data access --&gt; stream computing --&gt; data outputFlume不支持副本事件，节点崩溃导致数据丢失 kafka可以和Flume进行联合使用 Kafka的架构Kafka架构 名词 说明 Broker Kafka集群包含一个或者多个服务器，这种服务器被称为broker Topic 每条发布到Kafka集群的消息都有一个类别，该类别被称为topic Partition 每个topic包含一个或者多个partition Producer 负责发布消息到 Kafka broker Consumer 消息消费者，向Kafka broker读取消息的客户端 Consumer Group 每个consumer属于一个特定的consumer group 设计模式 生产者/消费者模式分析 发布/订阅模式分析 Kafka优点 delivery guarantee At most once: 数据可能会丢失，但绝不会重复传输 At least once:消息绝不会丢，但可能会重复传输 Exactly once: 每条消息传输一次且仅一次 Kafka的配置使用123456789101112131415161718192021setp1:启动zookeeper, kafkastep2:创建 Topicbin/kafka-topics.sh \\--create \\--zookeeper localhost:2181 \\--replication-facror 1 \\--partitions 1 \\--topic zhangyubin/kafka-topics.sh \\--list \\--zookeeper localhost:2181 \\step3:生产数据 bin/kafka-console-producer.sh \\--broker-list localhost:9092 \\--topic zhangyustep4:消费数据 bin/kafka-console-consumer.sh \\--zookeeper localhost:2181 \\--topic zhangyu \\--from-beginning","link":"/2020/04/12/BigData/Kafka%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"title":"Spark基础学习","text":"Spark简介什么是spark：基于内存的计算的大数据并行计算框架 spark 特点 速度比Hadoop快 通用性强 Transformations，Actions 容错性 Checkpoint, Lineage 内存中进行操作，适合对此迭代，不适合异步细粒度更新状态的应用，对于增量修改的应用模型不适用。数据量不是特点大，但是要求实时统计分析需求 spark生态系统组件简介spark生态图 BDAS数据分析站 构件 介绍 Spark Core 有向无环图（DAG）的分布是并行计算框架引入了RDD（Resilient Distributed Dataset）移动计算而非移动数据使用多线程池模型来减少task启动开销akka作为通讯框架 Spark Streaming 实时数据流及逆行高通量、容错处理的流式处理系统，可以对多种数据源进行类似Map、reduce和join等复杂操作，并将结果保存到外部文件系统中 Spark SQL 允许开发人员直接处理RDD，同时可以查询，同时更复杂的数据分析 MLBase/MLlib 专注于机器学习 GraphX 用于图和图之间的并行计算 Tachyon 高容错的分布式文件系统，允许文件以内存的速度在集群框架中进行可靠的共享 Mesos 一个集群管理七，提供了有效的、跨分布应用或框架的资源隔离和共享 BlinkDB 海量数据上运行交互式SQL查询的大国莫并行查询引擎 Spark 运行模式 Spark Local模式安装spark standalone模式安装spark编程模式spark shell","link":"/2020/04/13/BigData/Spark%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"title":"Sqoop介绍及库表操作","text":"Sqoop解决的问题：数据从关系型数据库到Hadoop生态系统的过程。 Sqoop官网：https://sqoop.apache.org sqoop支持的命令： 1sqoop help 列出数据库:检测环境是否正常 1234567xxxx:/apps$ \\&gt; sqoop list-databases \\&gt; -connect jdbc:mysql://localhost:3306 \\&gt; -username root \\&gt; -password strongs[output] mysql中数据库 列出表：列出mysqoop数据库中的表 1234567xxxx:/apps$ \\&gt; sqoop list-tables \\&gt; -connect jdbc:mysql://localhost:3306/mysqoop \\&gt; -username root \\&gt; -password strongs[output] mysqoop数据库中的表 把MySQL中数据导到HDFS中 12#查看Hadoop生态系统中的目录结构hadoop fs -lsr / 12345678xxxx:/apps$ \\&gt; sqoop import \\&gt; -connect jdbc:mysql://localhost:3306/mysqoop \\&gt; -username root \\&gt; -password strongs \\&gt; -table student \\&gt; -m 1 \\&gt; -target-dir /mysqoop/1/mystu 123456#查看Hadoop生态系统中的目录hadoop fs -ls /#查看Hadoop生态系统中mysqoop的目录结构hadoop fs -lsr /mysqoop#查看内容hadoop fs -text /mysqoop/1/mystu/part-m-00000 把MySQL中数据导到HBase中 123#启动hbasecd /apps/hbase/bin./start-hbase.sh 123#进入hbaseshell模式$ hbase shell$ list #查看表 12345678910xxxx:/apps$ \\&gt; sqoop import \\&gt; -connect jdbc:mysql://localhost:3306/mysqoop \\&gt; -username root \\&gt; -password strongs \\&gt; -table student \\&gt; -hbase-create-table \\&gt; -hbase-table mystu&gt; -column-family mycf \\&gt; -hbase-row-key id #MySQL中student表主键 12345#查看hbase变化$ hbase shelllist #列出表scan 'mystu' #扫描表","link":"/2020/04/04/BigData/Sqoop-1/"},{"title":"Sqoop数据导入与导出","text":"把HDFS中数据导到MySQL中 123456789xxxxxx:~$ \\&gt; sqoop export \\&gt; -connect jdbc:mysql://localhost:3306/mysqoop \\&gt; -username root \\&gt; -password strongs \\&gt; -table hdsftomysql \\&gt; -export-dir hdfs://localhost:9000/mysqoop/1/mystu/part-m-00000ps:MySQL中必须提前创建表结构关于其他数据源到处到MySQL中，暂无直接接口，可以依赖HDFS到MySQL的过程 Sqoop job 创建sqoop job 1234567891011$ sqoop job \\&gt; -create mysqoopjob1 \\&gt; -- import \\&gt; -connect jdbc:mysql://localhost:3306/mysqoop \\&gt; -username root \\&gt; -password strongs \\&gt; -table student \\&gt; -hbase-create-table \\&gt; -hbase-table mystu2 \\&gt; -column-famliy mycf \\&gt; -hbase-row-key id sqoop job其他操作 123456#查看jobsqoop job --list#执行jobsqoop job --exec mysqoopjob1#删除jobsqoop job --delete mysqoopjob1","link":"/2020/04/04/BigData/Sqoop-2/"},{"title":"卷积与卷积神经网络","text":"[","link":"/2020/04/16/DEEP-LEARNING/%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"回归问题及正则化","text":"线性回归模型及其求解方法利用训练数据，使用回归模型（如线性模型）去拟合变量之间的关系。因此训练任务就是利用数据，来学习模型中的参数parameter（如线性模型中的斜率和截距） 回归和分类的区别和联系 回归 分类 使用训练集推断输入x所对应的输出值，为连续实数 使用训练集推断输入x所对应的离散类别 联系1：利用回归模型进行分类：可将回归模型的输出离散化以及进行分类即 y = sign(f(x)) 联系2：利用分类模型进行回归：可利用分类模型的特点，输出其连续化的数值 线性模型（最常见最容易理解）非线性回归模型线性回归问题：已知一些数据，如何求里面的未知参数，给出一个最优解 概率解释求解参数 矩阵解法 梯度下降求解法 参数的矩阵解法参数的梯度下降求解法线性回归的梯度下降多元回归与多项式回归Sklearn的一元线性回归 在scikit-learn中，所有的估计其都带有fit()和predict()方法 例子+code 线性回归参数残差（residual）多元线性回归R方计算步骤多项式回归二次回归（Quadratic Regression）更高次的多项式回归损失函数的正则化向量范数矩阵范数线性回归的正则化线性回归正则化后的梯度更新方法岭回归中正则权重的作用通过交叉验证找到最佳参数alpha逻辑回归逻辑回归 ： 线性方程归一后损失函数优化方法：坐标下降法 对于非平滑函数CD法可能会遇到断点 使用Sklearn进行线性回归和二次划归的比较的程序示例","link":"/2020/04/10/DEEP-LEARNING/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/"},{"title":"深度学习——特征工程","text":"特征工程定义特征存在意义：数据和特征决定了机器学习的上线，而模型和算法只是逼近这个上限而已 深度学习也需要用到特征，需要对输入的特征进行组合变换等处理 自动分词自动分词就是将用自然语言书写的文章、句段经计算机处理后，以词为单位给以输出，为后续加工处理提供 先决条件 词根提取与词形标注词根提取：抽取词的词干或词根形式（不一定表达完整的语义） 词形还原：把词汇还原成一般形式（能表达完整语义） 词形规范化的两种重要方式 词性标注词性标注：为分词结果中的每一个单词标注一个正确的词性的程序 词性标注规范： 中文领域中尚无统一的标注标准，较为主流的是北大的词性标注集和宾州词性标注集。 句法分析句法分析：其基本任务时确定句子的句法结构或者句子中词汇之间的依存关系 NLTK: http://www.nltk.org/NLTK：在NLP领域中最常用的一个python库 提供的功能： Tokenization Stemming Tagging Parsing Text Processing API : http://text-processing.com/支持的功能: Stemming &amp; Lemmatization Sentiment Analysis Tagging and Chunk Extraction Phrase Extraction &amp; Named Entity Recognition 基于curl访问 Text Processing API下载： curl下载链接 curl-wim64 安装：只需要将文件解压到常用目录中即可 12345#进入bin目录F:\\My_Soft\\curl-7.69.1-win64-mingw\\bin#执行curl.exe#打开cmd,检查是否安装成功curl --version 通过curl访问 Text Processing API： 新建test.txt：内容是直接copy百度百科对美剧《犯罪心理》中Doctor.Reid该虚拟任务的介绍 输入相关命令 :进行情感分析 1curl -d &quot;text=test&quot; http://test-processing.com/api/sentiment 返回结果 结果解读：json文件包括情感标签和概率 neg: 负向情感的概率 neutral：中立情感概率 pos: 正向情感的概率 label：最后对该文本进行标注：选取概率最大的neutral TextBlob工具 ：功能更加强大中文处理工具：jieba安装： 1pip install jieba 功能： 分词 （并行分词、支持自定义词典） 词性标注 关键词提取","link":"/2020/04/06/DEEP-LEARNING/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"title":"特征工程——向量空间模型及文本相似度计算","text":"文档的向量化表示：BOW假设和VSM模型文本向量化的目的：便于计算文档时间的相似度 BOW(bag-of-words model):假设可以忽略文档内的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合。 VSM(Vector space model)：即向量空间模型。其是指在BOW假设下，将每个文档表示成同一向量空间的向量。 BOW-VSM栗子 停用词（stop words）停用词：非常常见且实际意义有限的词。几乎可能出现在所有场合，因而对某些应用比如信息检索、文本分类等区分度不大 停用词的过滤一般根据实际情况而定 N-gram模型 文档之间的欧氏距离 文档之间的余弦相似度 Tf-idf词条权重计算 Tf-idf词条权重计算举例","link":"/2020/04/07/DEEP-LEARNING/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"},{"title":"随意找了几张小狗U•ェ•*U的照片——可换做头像哟","text":"","link":"/2020/04/09/LIFE/%E9%9A%8F%E6%84%8F%E6%89%BE%E4%BA%86%E5%87%A0%E5%BC%A0%E5%B0%8F%E7%8B%97U%E2%80%A2%E3%82%A7%E2%80%A2-U%E7%9A%84%E7%85%A7%E7%89%87/"},{"title":"Linux常用命令——压缩与解压操作","text":"常用的压缩格式： .zip .rar .bz2 .tar.gz .tar.bz2 //按照压缩格式记相关命令相对方便 .zip格式压缩 &amp; 解压缩121. Linux 不严格区分压缩文件名，压缩文件不一定比源文件小(压缩文件大小 = 源文件大小 + 压缩格式)2. Linux中只要是压缩包，都以红色显示 123456#压缩文件zip new_file.zip org_file#压缩目录zip -r new_file.zip org_file#解压缩unzip name.zip（压缩文件） .gz格式压缩&amp;解压缩(windows适用)123456#压缩为.gz格式的压缩文件，源文件会消失gzip org_file#压缩为.gz格式的压缩文件，源文件保留gzip -c org_file &gt; new_file.gz#压缩目录下所有子文件，但不能压缩目录gzip -r catalogue 1234#解压缩文件gzip -d file.gzgunzip file.gzgunzip -r file.gz .bz2格式压缩（不能压缩目录）1234#压缩.bz2格式，不保留源文件bzip2 org_file#压缩后保留源文件bzip -k org_file","link":"/2020/03/23/Linux/Linuxux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94%E5%8E%8B%E7%BC%A9%E4%B8%8E%E8%A7%A3%E5%8E%8B%E6%93%8D%E4%BD%9C/"},{"title":"Linuxux常用命令——帮助命令","text":"帮助命令 man（manual）man 命令12#man - format and display the on-line manual pages#man [-acdfFhkKtwW] [--path] [-m system] [-p string] [-c config file] [-M pathlist] [-P pager] [-B bowser] [-H htmlpager] [-S section list] [section] name ... 查看快捷键或方式12输入 /-d #找到第一个dn #next 翻页 man的级别1234567891: 查看命令帮助2: 查看可被内核调用的函数的帮助3: 查看函数和函数库的帮助4: 查看特殊文件的帮助(主要是/dev目录下的文件)5: 查看配置文件的帮助6: 查看游戏的帮助7: 查看其他杂项的帮助8: 查看系统管理员可用命令的帮助9: 查看和内核相关文件的帮助 查看命令拥有那个级别的帮助12345man -f 命令 &lt;==&gt; whatis passwdeg: man -5 passwd #5等级帮助 man -1 passwd #1等级帮助 man -4 null man -8 ifconfig 查看和命令相关的所有帮助1man -k 命令 &lt;==&gt; apropos 命令 选项帮助123#获取命令选项的帮助命令 --helpeg: ls --help shell内部命令帮助1234567#获取shell内部命令help shell内部命令eg: whereis cd #确定是否是shell内部命令，判断（无有可执行文件，则为内部命令）help cd #内部命令 不能 man shell内部命令#help 只能获取内部命令 详细命令帮助info123456info 命令 -Enter: 进入子帮助页面（带有*号标记） -u: 进入上层页面 -n: 进入下一个帮助小节 -p: 进入上一个帮助小节 -q: 退出","link":"/2020/03/23/Linux/Linuxux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94%E5%B8%AE%E5%8A%A9%E5%91%BD%E4%BB%A4/"},{"title":"Linux——文件搜索命令","text":"文件搜索命令(比windows强大)文件搜索命令locate （搜索速度较快，但只能按照文件名搜索）locate命令格式 123456# 在后台数据库中按照文件名搜索，搜 索速度更快locate 文件名#locate命令搜索的后台数据库(Linux版本不同数据库的名称可能不同)/var/lib/mlocate#若要查询当天新建数据库，需要强制更新数据库updatedb /etc/updatedb.conf配置文件 12345678#开启搜索限制，YES表示以下规则全都生效PRUNE_BIND_MOUNTS = &quot;yes&quot;#搜索时，不搜索文件系统PRUNEFS=#搜索时，不搜索文件类型PRUNENAMES=#搜索时，不搜索的路径PRUNEPATHS= 命令搜索命令whereis、which1234#whereis 命令名 (只能查询系统文件) -b: 只查找可执行文件 -m: 只查找帮助文件#which 命令名 (位置+别名&lt;非所有命令&gt;) PATH环境变量12#定义用户操作环境的变量(定义的是系统搜索命令的路径)echo $PATH 文件搜索命令find (所有文件进行遍历)find 完全匹配 find命令 1234567#搜索文件find [搜索范围] [搜索条件]#避免大范围搜索，会非常消耗系统资源#find是在系统当中搜索符合体哦阿健的文件名。如果需要匹配，使用通配符匹配，通配符是完全匹配find / -name install.log#搜索以c结尾或者以d结尾find /root -name &quot;*[cd]&quot; Linux中的通配符 12345*： 匹配任意内容?: 匹配任意一个字符[]: 匹配任意一个中括号内的字符 find命令eg 12345678910111213141516171819202122232425262728293031#不区分大小写find /root -iname install.log#按照所有者搜索find /root -user root#查找没有所有者的文件find /root -nouser#查找10天前修改的文件find /var/log/ -mtime +10 -10:10天内修改文件 10: 10天当天修改的文件 +10: 10天前修改的文件 atime: 文件访问时间 ctime: 改变文件属性 mtime: 修改文件内容#查找文件大小是25KB的文件find . -size 25K -25k: 小于25KB的文件 25: 等于25kb的文件 +25: 大于25KB的文件#大小为数据块，根据扇区find /root -size 25#查找i节点是262422的文件ls -i find . -inum 262422#查找/etc/目录下，大于20kb并且小于50kb的文件find /etc -size _20k -a -size -50k -a and -o or#查找/etc/目录下，大于20kb并且小于50kb的文件，显示详细信息#-exec/-ok 命令 {} \\; 对搜索结果执行操作find /etc -size _20k -a -size -50k -exec ls -lh {} \\; 字符串搜索命令grepgrep 包含匹配 12345#在文件中匹配符合条件的字符串grep [option] string filename[option]: -i 忽略大小写 -v 排除指定字符串 grep-eg 123456#查看sizegrep &quot;size&quot; anaconda-ks.cfggrep -v &quot;size&quot; anaconda-ks.cfg[options]: -v: 取反 find与grep的区别12find:在系统中搜索符合条件的文件名，如果需要匹配使用通配符匹配，通配符是完全匹配grep:在文件中搜索符合条件的字符串，如果需要匹配，使用正则表达式进行匹配，正则表达式时包含匹配。","link":"/2020/03/24/Linux/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94%E6%96%87%E4%BB%B6%E6%90%9C%E7%B4%A2%E5%91%BD%E4%BB%A4/"},{"title":"Linux常用命令——文件目录操作","text":"命令格式命令提示符123456[root@localhost ~]# root: 当前登录用户 localhost: 主机名 ~: 当前所在目录(家目录) #: 超级用户的提示符 $: 普通用户提示符 命令格式12命令 [选项] [参数]注意：个别命令使用不遵循此格式，当有多个选项时，可以写在一起简化选项与完整选项 -a &lt;==&gt; --all 查询目录中内容:ls123456789101112131415161718192021222324ls [option] [file/catalogue][options]: -a:显示所有文件，包括隐藏文件 -l:显示详细信息 -d:查看目录属性 -h:人性化显示文件大小 -i:显示inode-rw-r--r-- -文件类型(-文件 d目录 l软连接文件) 其他文件类型: 块设备文件、字符设备文件、套接字文件和管道文件 rw- r-- r-- u所有者 g所属组 o其他人r:读w:写x:执行-rw-------. 1 root root 1207 1月 14 18:18 anaconda-ks.cfg . : ACL权限 1 : 引用计数，文件被调用次数 root: 文件主人 root: 所属组 1207: 文件大小(byte)(ls -lh:显示人能看得懂的) 1月 14 18:18: 最后一次修改时间 目录处理命令建立目录: mkdir12mkdir -p [目录名] -p 递归创建 切换所在目录: cd1234567cd [目录]简化操作： cd ~ :进入当前用户的家目录 cd : cd - :进入上次目录 cd ..:进入上一级目录 cd . :进入当前目录 查询所在目录位置: pwd1pwd :print working directory 删除空目录: rmdir1rmdir [目录名] : remove empty directories 删除文件或目录: rm1234rm -rf [文件或目录]rm -rf / #删除根目录下所有文件--自杀 -r: 删除目录 -f: 强制 复制命令 :cp123456cp [options] [orgfile/catelogue] [target_catelogue][options]: -r:复制目录 -p:连带文件属性复制 -d:若源文件时链接文件，则复制链接属性 -a:相当于 -pdr 剪切或改名命令 : mv1mv [原文件或者目录] [目标目录] 常见目录作用12345678910111213141516171819202122232425262728293031323334353637383940[root@localhost /]# lsbin :命令保存目录（普通用户就可以读取命令）sbin:命令保存目录(super_user才能使用)cgroup:etc:配置文件保存目录lib:系统库保存目录media:挂载目录mnt:系统挂载目录opt:root:超级用户的家目录selinux:sys:usr:boot:启动目录，启动相关文件dev:设备文件保存目录(别动)home:普通用户的家目录lost+found:misc:net:proc:直接写入内存srv:tmp:临时目录var:系统相关文档目录#proc和sys目录不能直接操作，这两个目录保存的时内存的过载点[root@localhost /]# ls usr/bin:etc:games:include:lib:libexec:local:sbin:share:src:tmp:#根目录下的bin和sbin,usr目录下的bin和sbin,这四个目录都是用来保存系统命令的 链接命令连接命令:ln123ln -s [原文件] [目标文件][options]: -s 创建软连接 硬链接特征12341.拥有相同的i节点和存储block块，可以看作时同一个文件2.可通过i节点识别3.不能跨分区4.不能针对目录使用 软连接特征123451.类似windows快捷方式2.软连接拥有自己的I节点和block块，但是数据块中只保存文件的文件名和I节点号，但没有实际的文件书数据3.lrwxrwxrwx l软连接：软连接的文件权限都为rwxrwxrwx4.修改任意文件，另一个都改变5.删除原文件，软连接不能使用","link":"/2020/03/25/Linux/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/"},{"title":"Linux常用命令——用户操作","text":"查看登录用户命令12345678910w 用户名命令输出： USER:登录的用户名 TTY:登陆终端 FROM:IP地址登录 LOGIN@:登陆时间 IDLE:用户闲置时间 JCPU:指的时和该终端连接的所有进程的占用时间。不包括后台时间，但却包括当前正在运行的后台作业所占用的时间。 PCPU:指当前进程所占用的时间 WHAT:当前正在运行的命令 查看登录用户信息12345who 用户名命令输出: -用户名 -登陆终端 -登陆时间(登陆来源IP地址) 查询当前登录和过去的登陆的用户信息12345678last#last命令默认是读取/var/log/wtmp文件数据命令输出： -用户名 -登陆终端 -登录IP -登陆时间 -退出时间(在线时间)","link":"/2020/03/25/Linux/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94%E7%94%A8%E6%88%B7%E6%93%8D%E4%BD%9C/"},{"title":"Linux常用命令——系统介绍","text":"简介开源软件应用领域学习方法与WIN的不同123456789Linux严格区分大小写Linux中所有内容跟以文件形式保存，包括硬件Linux不靠扩展名区分文件类型 压缩包:&quot;*.gz&quot; &quot;*.bz2&quot; &quot;*.tar.bz2&quot; &quot;*.tgz&quot;等 二进制软件包:&quot;.rpm&quot; 网页文件:&quot;*.html&quot; &quot;*.php&quot; 脚本文件:&quot;*.sh&quot; 配置文件:&quot;*.conf&quot;Windows下的程序不能直接在Linux中安装和运行 字符界面的优势121.占用的系统资源更少2.减少了出错、被攻击的可能","link":"/2020/03/25/Linux/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/"},{"title":"基于维基百科中文语料库的Word2Vec模型训练","text":"说明：该博客代码参考于参考博客：使用中文维基百科语料库+opencc+jieba+gensim训练一个word2vec模型参考博客：使用中文维基百科训练word2vec模型 零、 模型训练环境 Windows10-X64 、 python2.7 、 python3.6 pip install jieba pip install gensim 一、下载维基百科语料库数据下载地址该博客使用的是 24-Nov-2019 的语料库 二、 利用WikiExtractor.py抽取正文WikiExtractor.py Copy链接直接把py文件中的源码直接复制到新的 .py 中即可。下图是模型训练相关文件：执行命令 1python WiKiExtractor.py -b 500M -o zhwiki zhwiki-20191120-pages-articles-multistream.xml.bz2 我们最终会生成zhwiki文件夹得到三个文本文件 ./zhwiki/AA/wiki_00./zhwiki/AA/wiki_01./zhwiki/AA/wiki_02 三、 中文繁体转换成简体简体直接使用开源项目转换工具opencc转换工具下载地址黑框框起来的是我下载的版本 执行文件添加到环境变量中：方法1. 解压后将该工具bin目录添加到环境变量中方法2. 有些工具解压后灭有 bin 目录，我们首先找到opencc.exe可执行文件，将其路径添加到环境变量中 运行命令我是在该路径中运行的命令，为了方便我先将之前的三个文件拷贝到该路径下，执行完后再拷贝到./zhwiki/ 下。 123.\\opencc -i wiki_00 -o zh_wiki_00 -c /path/to/t2s.json.\\opencc -i wiki_01 -o zh_wiki_01 -c /path/to/t2s.json.\\opencc -i wiki_02 -o zh_wiki_02 -c /path/to/t2s.json 最终我把生成的zh_wiki_00等文件拷贝到 /zhwiki/ 中 四、符号处理 ，整合文件python 2.7 环境中运行exec_sum.py 1234567891011121314151617181920212223242526#!python2import reimport sysimport codecsdef myfun(input_file): p1 = re.compile(ur'-\\{.*?(zh-hans|zh-cn):([^;]*?)(;.*?)?\\}-') p2 = re.compile(ur'[（\\(][，；。？！\\s]*[）\\)]') p3 = re.compile(ur'[「『]') p4 = re.compile(ur'[」』]') outfile = codecs.open('std_zh_wiki', 'a+', 'utf-8') with codecs.open(input_file, 'r', 'utf-8') as myfile: for line in myfile: line = p1.sub(ur'\\2', line) line = p2.sub(ur'', line) line = p3.sub(ur'“', line) line = p4.sub(ur'”', line) outfile.write(line) outfile.close()if __name__ == '__main__': if len(sys.argv) != 2: print(\"Usage: python script.py inputfile\") sys.exit() reload(sys) sys.setdefaultencoding('utf-8') input_file = sys.argv[1] myfun(input_file) 方法一：将三个文件处理后追加到一个文件中即 std_zh_wiki 123python .\\exec_sum.py zh_wiki_00python .\\exec_sum.py zh_wiki_01python .\\exec_sum.py zh_wiki_02 方法二：我是使用 pycharm IDE 运行了 三次该文件，每次运行修改 input_file(zh_wiki_00、zh_wiki_01、zh_wiki_02) (更改python环境更方便) 五、jieba 分词操作exec_cut.py 123456789101112131415161718192021222324252627282930313233343536import logging, jieba, os, redef get_stopwords(): logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO) # 加载停用词表 stopword_set = set() with open(\"stopwords.txt\", 'r') as stopwords: for stopword in stopwords: stopword_set.add(stopword.strip(\"\\n\")) return stopword_setdef parse_zhwiki(read_file_path, save_file_path): # 过滤掉&lt;doc&gt; regex_str = \"[^&lt;doc.*&gt;$]|[^&lt;/doc&gt;$]\" file = open(read_file_path, \"r\") output = open(save_file_path, \"w+\") content_line = file.readline() stopwords = get_stopwords() article_contents = \"\" while content_line: match_obj = re.match(regex_str, content_line) content_line = content_line.strip(\"\\n\") if len(content_line) &gt; 0: if match_obj: words = jieba.cut(content_line, cut_all=False) for word in words: if word not in stopwords: article_contents += word + \" \" else: if len(article_contents) &gt; 0: output.write(article_contents.encode('utf-8','ignore')+ \"\\n\") article_contents = \"\" content_line = file.readline() output.close()parse_zhwiki('./std_zh_wiki', './cut_std_zh_wiki') 该操作也是在 python 2.7环境下操作 分词时间相对较长，请耐心等待 六、 训练Word2Vec模型1234567from gensim.models import word2vecimport logginglogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)sentences = word2vec.LineSentence('./cut_std_zh_wiki')model = word2vec.Word2Vec(sentences,size=200,window=5,min_count=5,workers=4)model.save('WikiCHModel') 训练完后，会生成三个文件 如果python3环境下出现编码问题，请通过某些工具将文件转换成utf-8编码 如果无法更改编码，请使用python2.7 七、 模型测试123456from gensim.models import word2vecimport loggingfrom gensim import modelsmodel = word2vec.Word2Vec.load('WikiCHModel')print(model.wv.similarity(\"儿童\", '狗')) #两个词的相关性 若博客中内容存在问题，可在评论处留言，本人将及时更正相关内容","link":"/2020/03/09/NLP/nlp1/"},{"title":"解决NLTK语料库下载出错及nltk_data路径等问题","text":"一、解决NLTK语料库下载问题NLTK有许多可供使用的语料库，但直接通过官网下载会出现某些问题下载语料库代码 12import nltknltk.download(\"all\") 代码中添加的参数可参考官方链接 问题一：下载速度过慢，考验你的耐心 问题二：下载速度过慢导致下载中止，再次运行命令会导致下图情况 解决方案：通过百度资源下载nltk_data链接：nltk_data提取码：ucun 二、nltk_data路径问题1.直接解压在C盘根目录下(C:\\nltk_data)测试代码 1234from nltk.book import *from nltk.corpus import reutersfiles = reuters.fileids()print(files) 2.放在任意目录下:测试代码 123456from nltk import datafrom nltk.corpus import reuters#每次访问数据需要添加数据至路径当中data.path.append(r\"F:\\About-Python\\NLP_env_1\\note\\nltk_data\")files = reuters.fileids()print(files) 输出结果遇见问题级解决方案持续更新中","link":"/2020/03/09/NLP/nlp_nltk/"},{"title":"深度学习.pdf","text":"","link":"/2020/03/15/books/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%82%E8%80%83%E4%B9%A6%E7%B1%8D/"},{"title":"Windows 10下进行MySQL数据文件的转移","text":"​ 在进行数据分析的过程中，“基石”便是数据，可在使用过程中，存储在某个盘中的数据会不断累积，特别是MySQL的默认安装路径为C盘(C:\\Program Files\\MySQL\\MySQL Server 8.0)，将会导致C盘内存越用越小。同时个人因为各种需求也需要进行数据文件的转移。 转移文件12345** windows下数据文件为隐藏文件 基本都在 C:\\ProgramData 该路径下 **Step1:找到 C:\\ProgramData\\MySQL\\MySQL Server 8.0 下的 Data 文件Setp2: 复制Data文件到 转移目标路径下 F:\\Data\\MySQL Server 8.0\\ --&gt; F:\\Data\\MySQL Server 8.0\\Data 关闭MySQL服务1net stop mysql80 更改配置——数据存储路径&lt; img src=”1.PNG” alt=”post-cover”&gt; 1234567Step1:找到 C:\\ProgramData\\MySQL\\MySQL Server 8.0\\my.ini 打开文件进行编辑Setp2: # Path to the database root#datadir=C:/ProgramData/MySQL/MySQL Server 8.0/Data ⬇ (将上面原路径注释，插入转移的目标路径)datadir=F:/Data/MySQL Server 8.0/Data 启动MySQL服务1net start mysql80 测试通过新建DataBases，查看文件是否新建文件 123#进入mysqlmysql -u root -pcreate databases test;","link":"/2020/03/19/databases/mysql-data%E8%BF%81%E7%A7%BB/"},{"title":"python-math标准库的使用","text":"如何查看math标准库中的相关方法: 12import mathprint(dir(math)) 12345['__doc__', '__loader__', '__name__', '__package__', '__spec__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'copysign', 'cos', 'cosh', 'degrees', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'gcd', 'hypot', 'inf', 'isclose', 'isfinite','isinf', 'isnan', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'log2', 'modf', 'nan', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'tau', 'trunc'] ps:结果的是以列表的形式呈现出来的 库中方法分类(python3.6) 类别 个数 详情 三角函数类 13 cos(),cosh(),acos()acosh(),sin(),sinh(),asin(),asinh(),tan(),tanh(),atan(),atanh(),atan2() 常数类 3 pi,e,tau(2*pi) 常非数类 2 inf&lt;无穷&gt;,nan&lt;不是数&gt; 取整 3+1 ceil(),floor(),round()&lt;round()为python内置函数直接调用即可&gt;trunc()&lt;将小数部分直接砍掉&gt;–&gt;trunc(2.58585458678465)==2 is&lt;…&gt;判断函数 4 isclose(),isinf(),ifnan(),isfinite() 常规计算操作 7 求和:fsum();开方:sqrt();最小公约数:gcd();阶乘:factorial()模运算:fmod(x,y),modf()&lt;modf()展现小数部分&gt;绝对值:fabs(),abs() &lt;abs()为python内置函数直接调用&gt; 幂运算 6 exp(),frexp(),frex(),expm1(),pow(),ldexp(x,i)==x(2*i) 对数 4 log(),log10(),loglp(),log2() 魔法方法 5 '__doc__', '__loader__', '__name__', '__package__', '__spec__' 判断函数正确性 2 erf(),erfc() 复杂数学公式实现 5 gamma函数:gamma(),lgamma()计算三角形斜边函数:hypot()弧度,角度之间转换:degrees()&lt;弧-&gt;角&gt;,radians()&lt;角-&gt;弧&gt; 方法使用过程中的注意事项1.相同功能的函数其准确性的比较: exp(2) &gt; e**2 ; log10(x) &gt; log(x,10)2.相同功能函数其执行速度比较: abs(x) &gt; fabs(x) (x必须为数值型)3.python-math库 学习手册(官方资料) ps:整理过程中有不恰当的地方可以在下方留言(＾Ｕ＾)ノ~ＹＯ ?","link":"/2020/03/09/python/python1/"},{"title":"2020-3-9 (From Criminal Minds)","text":"Better to write for youself and have no public.Then to write for the public and have noself.","link":"/2020/03/09/study%20english/2020-3-9/"},{"title":"2020-3-18","text":"Pain is the breaking of the shell that encloses your understanding.","link":"/2020/03/18/study%20english/2020-3-18/"},{"title":"Hadoop——MapReduce学习","text":"本章从MapReduce的原理、工作机制、应用开发、设计模式、算法时间和性能调优进行了详细的介绍 目标： 1、了解MapReduce原理(map函数、reduce函数、shuffle过程) 2、掌握MapReduce相关算法 所有实例的数据&amp;依赖包hadoop2lib.tar.gz——百度云链接 提取码：hsxq MapReduce学习——习题1.对MapReduce的体系结构，以下说法正确的是： A.分布式编程架构 B.以数据为中心，更看重吞吐量 C.分而治之的思想 D.将一个任务分解成多个任务 2.MapReduce为了保证任务的正常执行，采用（）等多种容错机制 A.重复执行 B.重新开始整个任务 C.推测执行 D.直接丢弃执行效率低的作业 3.对Hadoop中JobTacker的工作角色，以下说法正确的是（） A.作业调度 B.分配任务 C.监控CPU运行效率 D.监控任务执行进度 4.在Hadoop中每个应用程序被表示成一个作业，每个作业又被分成多个任务，JobTracker的负责作业的分解、状态监控以及资源管理 5.Map的主要工作是将任务分解成多个任务 MapReduce实例——WordCount任务目标1.准确理解Mapreduce的设计原理 2.熟练掌握WordCount程序代码编写 3.学会自己编写WordCount程序进行词频统计 相关知识MapReduce采用的是“分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个从节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单来说，MapReduce就是”任务的分解与结果的汇总“。 1.MapReduce的工作原理 在分布式计算中，MapReduce框架负责处理了并行编程里分布式存储、工作调度，负载均衡、容错处理以及网络通信等复杂问题，现在我们把处理过程高度抽象为Map与Reduce两个部分来进行阐述，其中Map部分负责把任务分解成多个子任务，Reduce部分负责把分解后多个子任务的处理结果汇总起来，具体设计思路如下。 （1）Map过程需要继承org.apache.hadoop.mapreduce包中Mapper类，并重写其map方法。通过在map方法中添加两句把key值和value值输出到控制台的代码，可以发现map方法中输入的value值存储的是文本文件中的一行（以回车符为行结束标记），而输入的key值存储的是该行的首字母相对于文本文件的首地址的偏移量。然后用StringTokenizer类将每一行拆分成为一个个的字段，把截取出需要的字段（本实验为买家id字段）设置为key，并将其作为map方法的结果输出。 （2）Reduce过程需要继承org.apache.hadoop.mapreduce包中Reducer类，并重写其reduce方法。Map过程输出的&lt;key,value&gt;键值对先经过shuffle过程把key值相同的所有value值聚集起来形成values，此时values是对应key字段的计数值所组成的列表，然后将&lt;key,values&gt;输入到reduce方法中，reduce方法只要遍历values并求和，即可得到某个单词的总次数。 在main()主函数中新建一个Job对象，由Job对象负责管理和运行MapReduce的一个计算任务，并通过Job的一些方法对任务的参数进行相关的设置。本实验是设置使用将继承Mapper的doMapper类完成Map过程中的处理和使用doReducer类完成Reduce过程中的处理。还设置了Map过程和Reduce过程的输出类型：key的类型为Text，value的类型为IntWritable。任务的输出和输入路径则由字符串指定，并由FileInputFormat和FileOutputFormat分别设定。完成相应任务的参数设定后，即可调用job.waitForCompletion()方法执行任务，其余的工作都交由MapReduce框架处理。 2.MapReduce框架的作业运行流程 （1）ResourceManager：是YARN资源控制框架的中心模块，负责集群中所有资源的统一管理和分配。它接收来自NM(NodeManager)的汇报，建立AM，并将资源派送给AM(ApplicationMaster)。 （2）NodeManager：简称NM，NodeManager是ResourceManager在每台机器上的代理，负责容器管理，并监控他们的资源使用情况（cpu、内存、磁盘及网络等），以及向ResourceManager提供这些资源使用报告。 （3）ApplicationMaster：以下简称AM。YARN中每个应用都会启动一个AM，负责向RM申请资源，请求NM启动Container，并告诉Container做什么事情。 （4）Container：资源容器。YARN中所有的应用都是在Container之上运行的。AM也是在Container上运行的，不过AM的Container是RM申请的。Container是YARN中资源的抽象，它封装了某个节点上一定量的资源（CPU和内存两类资源）。Container由ApplicationMaster向ResourceManager申请的，由ResouceManager中的资源调度器异步分配给ApplicationMaster。Container的运行是由ApplicationMaster向资源所在的NodeManager发起的，Container运行时需提供内部执行的任务命令（可以是任何命令，比如java、Python、C++进程启动命令均可）以及该命令执行所需的环境变量和外部资源（比如词典文件、可执行文件、jar包等）。 另外，一个应用程序所需的Container分为两大类，如下： ①运行ApplicationMaster的Container：这是由ResourceManager（向内部的资源调度器）申请和启动的，用户提交应用程序时，可指定唯一的ApplicationMaster所需的资源。 ②运行各类任务的Container：这是由ApplicationMaster向ResourceManager申请的，并为了ApplicationMaster与NodeManager通信以启动的。 以上两类Container可能在任意节点上，它们的位置通常而言是随机的，即ApplicationMaster可能与它管理的任务运行在一个节点上。 任务步骤12345678910111213141516171819#step1: 启动Hadoop#step2: 创建目录，下载数据文件#step3: 将linux本地/data/mapreduce1/buyer_favorite1，上传到HDFS上的/mymapreduce1/in目录下hadoop fs -mkdir -p /mymapreduce1/in hadoop fs -put /data/mapreduce1/buyer_favorite1 /mymapreduce1/in #step4: 打开Eclipse，新建Java Project项目,并将项目名设置为mapreduce1;在项目名mapreduce1下，新建package包,并将包命名为mapreduce;在创建的包mapreduce下，新建类,并将类命名为WordCount.#step5: 添加项目所需依赖的jar包,右键单击项目名，新建一个目录hadoop2lib，用于存放项目所需的jar包。#step6: 编写Java代码#step7: 在WordCount类文件中，单击右键=&gt;Run As=&gt;Run on Hadoop选项，将MapReduce任务提交到Hadoop中#step8: 待执行完毕后，打开终端或使用hadoop eclipse插件，查看hdfs上，程序输出的实验结果hadoop fs -ls /mymapreduce1/out hadoop fs -cat /mymapreduce1/out/part-r-00000 完整代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package mapreduce; import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class WordCount { public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { Job job = Job.getInstance(); job.setJobName(\"WordCount\"); job.setJarByClass(WordCount.class); job.setMapperClass(doMapper.class); job.setReducerClass(doReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); Path in = new Path(\"hdfs://localhost:9000/mymapreduce1/in/buyer_favorite1\"); Path out = new Path(\"hdfs://localhost:9000/mymapreduce1/out\"); FileInputFormat.addInputPath(job, in); FileOutputFormat.setOutputPath(job, out); System.exit(job.waitForCompletion(true) ? 0 : 1); } public static class doMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;{ public static final IntWritable one = new IntWritable(1); public static Text word = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException { StringTokenizer tokenizer = new StringTokenizer(value.toString(), \"\\t\"); word.set(tokenizer.nextToken()); context.write(word, one); } } public static class doReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ private IntWritable result = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable value : values) { sum += value.get(); } result.set(sum); context.write(key, result); } } } MapReduce实例——去重任务目标1.准确理解MapReduce去重的设计原理 2.熟练掌握MapReduce去重的程序编写 3.学会自己编写MapReduce去重代码解决实际问题 相关知识“数据去重”主要是为了掌握和利用并行化思想来对数据进行有意义的筛选。统计大数据集上的数据种类个数、从网站日志中计算访问地等这些看似庞杂的任务都会涉及数据去重。 数据去重的最终目标是让原始数据中出现次数超过一次的数据在输出文件中只出现一次。在MapReduce流程中，map的输出&lt;key,value&gt;经过shuffle过程聚集成&lt;key,value-list&gt;后交给reduce。我们自然而然会想到将同一个数据的所有记录都交给一台reduce机器，无论这个数据出现多少次，只要在最终结果中输出一次就可以了。具体就是reduce的输入应该以数据作为key，而对value-list则没有要求（可以设置为空）。当reduce接收到一个&lt;key,value-list&gt;时就直接将输入的key复制到输出的key中，并将value设置成空值，然后输出&lt;key,value&gt;。 MaprReduce去重流程如下图所示： 任务内容现有一个某电商网站的数据文件，名为buyer_favorite1，记录了用户收藏的商品以及收藏的日期，文件buyer_favorite1中包含（用户id，商品id，收藏日期）三个字段，数据内容以“\\t”分割，要求用Java编写MapReduce程序，根据商品id进行去重，统计用户收藏商品中都有哪些商品被收藏。 任务步骤1.启动Hadoop 1234#切换到hadoop的sbin目录下cd /apps/hadoop/sbin#启动Hadoop./start-all.sh 2.新建目录，下载数据包并下载和解压依赖包hadoop2lib.tar.gz 123456#新建目录是为了方便进行项目数据的整理mkdir -p /data/mapreduce2cd /data/mapreduce2#下载并解压压缩包，点击文章开头的百度云链接获取全部数据以及依赖包gunzip buyer_favotite1.bztar xzvf hadoop2lib.tar.gz 去重实例数据包——buyer_favorite1 提取码：9t51 3.在HDFS上创建/in目录，并移动数据文件至in目录中 12hadoop fs -mkdir -p /mymapreduce2/inhadoop fs -put /data/mapreduce2/buyer_favorite1 /mymapreduce2/in 4.创建java项目，并编写程序且执行 1234567setp1:新建Java Project项目，项目名为mapreduce2step2:在mapreduce2项目下新建包，包名为mapreducestep3:在mapreduce包下新建类，类名为Filterstep4:添加项目所需依赖的jar包右键项目，新建一个文件夹，命名为：hadoop2lib,用于存放项目所需的jar包step5:将/data/mapreduce2目录下，hadoop2lib目录中的jar包，拷贝到eclipse中mapreduce2项目的hadoop2lib目录下step6:选中所有项目hadoop2lib目录下所有jar包，并添加到Build Path中step:7在Filter类文件中，右键并点击=&gt;Run As=&gt;Run on Hadoop选项，将MapReduce任务提交到Hadoop中 项目文件层级关系如下： 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182//Map代码public static class Map extends Mapper&lt;Object , Text , Text , NullWritable&gt; //map将输入中的value复制到输出数据的key上，并直接输出 { private static Text newKey=new Text(); //从输入中得到的每行的数据的类型 public void map(Object key,Text value,Context context) throws IOException, InterruptedException //实现map函数 { //获取并输出每一次的处理过程 String line=value.toString(); System.out.println(line); String arr[]=line.split(\"\\t\"); newKey.set(arr[1]); context.write(newKey, NullWritable.get()); System.out.println(newKey); } } //map阶段采用Hadoop的默认的作业输入方式，把输入的value用split()方法截取，截取出的商品id字段设置为key,设置value为空，然后直接输出&lt;key,value&gt;。//reduce端代码view plain copypublic static class Reduce extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt;{ public void reduce(Text key,Iterable&lt;NullWritable&gt; values,Context context) throws IOException, InterruptedException //实现reduce函数 { context.write(key,NullWritable.get()); //获取并输出每一次的处理过程 } } //map输出的&lt;key,value&gt;键值对经过shuffle过程，聚成&lt;key,value-list&gt;后，会交给reduce函数。reduce函数,不管每个key 有多少个value，它直接将输入的值赋值给输出的key，将输出的value设置为空，然后输出&lt;key,value&gt;就可以了。//完整代码view plain copypackage mapreduce; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; public class Filter{ public static class Map extends Mapper&lt;Object , Text , Text , NullWritable&gt;{ private static Text newKey=new Text(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException{ String line=value.toString(); System.out.println(line); String arr[]=line.split(\"\\t\"); newKey.set(arr[1]); context.write(newKey, NullWritable.get()); System.out.println(newKey); } } public static class Reduce extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt;{ public void reduce(Text key,Iterable&lt;NullWritable&gt; values,Context context) throws IOException, InterruptedException{ context.write(key,NullWritable.get()); } } public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException{ Configuration conf=new Configuration(); System.out.println(\"start\"); Job job =new Job(conf,\"filter\"); job.setJarByClass(Filter.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); Path in=new Path(\"hdfs://localhost:9000/mymapreduce2/in/buyer_favorite1\"); Path out=new Path(\"hdfs://localhost:9000/mymapreduce2/out\"); FileInputFormat.addInputPath(job,in); FileOutputFormat.setOutputPath(job,out); System.exit(job.waitForCompletion(true) ? 0 : 1); } } 5.查看实验结果 12hadoop fs -ls /mymapreduce2/out hadoop fs -cat /mymapreduce2/out/part-r-00000 MapReduce实例——排序任务目标1.准确理解Mapreduce排序的实验原理 2.熟练掌握Mapreduce排序的程序代码编写 3.培养编写MapReduce排序代码解决问题的能力 相关知识Map、Reduce任务中Shuffle和排序的过程图如下： 流程分析： 1.Map端： （1）每个输入分片会让一个map任务来处理，默认情况下，以HDFS的一个块的大小（默认为64M）为一个分片，当然我们也可以设置块的大小。map输出的结果会暂且放在一个环形内存缓冲区中（该缓冲区的大小默认为100M，由io.sort.mb属性控制），当该缓冲区快要溢出时（默认为缓冲区大小的80%，由io.sort.spill.percent属性控制），会在本地文件系统中创建一个溢出文件，将该缓冲区中的数据写入这个文件。 （2）在写入磁盘之前，线程首先根据reduce任务的数目将数据划分为相同数目的分区，也就是一个reduce任务对应一个分区的数据。这样做是为了避免有些reduce任务分配到大量数据，而有些reduce任务却分到很少数据，甚至没有分到数据的尴尬局面。其实分区就是对数据进行hash的过程。然后对每个分区中的数据进行排序，如果此时设置了Combiner，将排序后的结果进行Combia操作，这样做的目的是让尽可能少的数据写入到磁盘。 （3）当map任务输出最后一个记录时，可能会有很多的溢出文件，这时需要将这些文件合并。合并的过程中会不断地进行排序和combia操作，目的有两个：①尽量减少每次写入磁盘的数据量。②尽量减少下一复制阶段网络传输的数据量。最后合并成了一个已分区且已排序的文件。为了减少网络传输的数据量，这里可以将数据压缩，只要将mapred.compress.map.out设置为true就可以了。 （4）将分区中的数据拷贝给相对应的reduce任务。有人可能会问：分区中的数据怎么知道它对应的reduce是哪个呢？其实map任务一直和其父TaskTracker保持联系，而TaskTracker又一直和JobTracker保持心跳。所以JobTracker中保存了整个集群中的宏观信息。只要reduce任务向JobTracker获取对应的map输出位置就ok了哦。 到这里，map端就分析完了。那到底什么是Shuffle呢？Shuffle的中文意思是“洗牌”，如果我们这样看：一个map产生的数据，结果通过hash过程分区却分配给了不同的reduce任务，是不是一个对数据洗牌的过程呢？ 2.Reduce端： （1）Reduce会接收到不同map任务传来的数据，并且每个map传来的数据都是有序的。如果reduce端接受的数据量相当小，则直接存储在内存中（缓冲区大小由mapred.job.shuffle.input.buffer.percent属性控制，表示用作此用途的堆空间的百分比），如果数据量超过了该缓冲区大小的一定比例（由mapred.job.shuffle.merge.percent决定），则对数据合并后溢写到磁盘中。 （2）随着溢写文件的增多，后台线程会将它们合并成一个更大的有序的文件，这样做是为了给后面的合并节省时间。其实不管在map端还是reduce端，MapReduce都是反复地执行排序，合并操作，现在终于明白了有些人为什么会说：排序是hadoop的灵魂。 （3）合并的过程中会产生许多的中间文件（写入磁盘了），但MapReduce会让写入磁盘的数据尽可能地少，并且最后一次合并的结果并没有写入磁盘，而是直接输入到reduce函数。 熟悉MapReduce的人都知道：排序是MapReduce的天然特性！在数据达到reducer之前，MapReduce框架已经对这些数据按键排序了。但是在使用之前，首先需要了解它的默认排序规则。它是按照key值进行排序的，如果key为封装的int为IntWritable类型，那么MapReduce按照数字大小对key排序，如果Key为封装String的Text类型，那么MapReduce将按照数据字典顺序对字符排序。 了解了这个细节，我们就知道应该使用封装int的Intwritable型数据结构了，也就是在map这里，将读入的数据中要排序的字段转化为Intwritable型，然后作为key值输出（不排序的字段作为value）。reduce阶段拿到&lt;key，value-list&gt;之后，将输入的key作为输出的key，并根据value-list中的元素的个数决定输出的次数。 任务内容在电商网站上，当我们进入某电商页面里浏览商品时，就会产生用户对商品访问情况的数据 ，名为goods_visit1，goods_visit1中包含（商品id ，点击次数）两个字段，内容以“\\t”分割，由于数据量很大，所以为了方便统计我们只截取它的一部分数据，内容如下： 商品id 点击次数 1010037 100 1010102 100 1010152 97 1010178 96 1010320 103 1010510 104 要求我们编写mapreduce程序来对商品点击次数有低到高进行排序。 实验结果数据如下： 商品id 点击次数 1010178 96 1010152 97 1010037 100 1010102 100 1010320 103 1010510 104 任务步骤1.启动Hadoop 1234#切换到hadoop的sbin目录下cd /apps/hadoop/sbin#启动Hadoop./start-all.sh 2.新建目录，下载数据包并下载和解压依赖包hadoop2lib.tar.gz 123456#新建目录是为了方便进行项目数据的整理mkdir -p /data/mapreduce3cd /data/mapreduce3#下载并解压压缩包，点击文章开头的百度云链接获取全部数据以及依赖包gunzip goods_visit1.gztar xzvf hadoop2lib.tar.gz 3.在HDFS上创建/in目录，并移动数据文件至in目录中 12hadoop fs -mkdir -p /mymapreduce3/inhadoop fs -put /data/mapreduce3/goods_visit1 /mymapreduce3/in 4.创建java项目，并编写程序且执行 1234567setp1:新建Java Project项目，项目名为mapreduce3step2:在mapreduce3项目下新建包，包名为mapreducestep3:在mapreduce包下新建类，类名为OneSortstep4:添加项目所需依赖的jar包右键项目，新建一个文件夹，命名为：hadoop2lib,用于存放项目所需的jar包step5:将/data/mapreduce3目录下，hadoop2lib目录中的jar包，拷贝到eclipse中mapreduce3项目的hadoop2lib目录下step6:选中所有项目hadoop2lib目录下所有jar包，并添加到Build Path中step:7在OneSort类文件中，右键并点击=&gt;Run As=&gt;Run on Hadoop选项，将MapReduce任务提交到Hadoop中 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980//Java代码，并描述其设计思路//在MapReduce过程中默认就有对数据的排序。它是按照key值进行排序的，如果key为封装int的IntWritable类型，那么MapReduce会按照数字大小对key排序，如果Key为封装String的Text类型，那么MapReduce将按照数据字典顺序对字符排序。在本例中我们用到第一种，key设置为IntWritable类型，其中MapReduce程序主要分为Map部分和Reduce部分。//Map部分代码public static class Map extends Mapper&lt;Object,Text,IntWritable,Text&gt;{ private static Text goods=new Text(); private static IntWritable num=new IntWritable(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException{ String line=value.toString(); String arr[]=line.split(\"\\t\"); num.set(Integer.parseInt(arr[1])); goods.set(arr[0]); context.write(num,goods); } } //在map端采用Hadoop默认的输入方式之后，将输入的value值用split()方法截取，把要排序的点击次数字段转化为IntWritable类型并设置为key，商品id字段设置为value，然后直接输出&lt;key,value&gt;。map输出的&lt;key,value&gt;先要经过shuffle过程把相同key值的所有value聚集起来形成&lt;key,value-list&gt;后交给reduce端。//Reduce部分代码public static class Reduce extends Reducer&lt;IntWritable,Text,IntWritable,Text&gt;{ private static IntWritable result= new IntWritable(); //声明对象result public void reduce(IntWritable key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException{ for(Text val:values){ context.write(key,val); } } } //完整代码package mapreduce; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; public class OneSort { public static class Map extends Mapper&lt;Object , Text , IntWritable,Text &gt;{ private static Text goods=new Text(); private static IntWritable num=new IntWritable(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException{ String line=value.toString(); String arr[]=line.split(\"\\t\"); num.set(Integer.parseInt(arr[1])); goods.set(arr[0]); context.write(num,goods); } } public static class Reduce extends Reducer&lt; IntWritable, Text, IntWritable, Text&gt;{ private static IntWritable result= new IntWritable(); public void reduce(IntWritable key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException{ for(Text val:values){ context.write(key,val); } } } public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException{ Configuration conf=new Configuration(); Job job =new Job(conf,\"OneSort\"); job.setJarByClass(OneSort.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(Text.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); Path in=new Path(\"hdfs://localhost:9000/mymapreduce3/in/goods_visit1\"); Path out=new Path(\"hdfs://localhost:9000/mymapreduce3/out\"); FileInputFormat.addInputPath(job,in); FileOutputFormat.setOutputPath(job,out); System.exit(job.waitForCompletion(true) ? 0 : 1); } } 5.查看实验结果 12hadoop fs -ls /mymapreduce3/out hadoop fs -cat /mymapreduce3/out/part-r-00000 之后实例的步骤与前三个步骤一致，不同的便是代码，此后就直接码上代码，省略详细步骤了 MapReduce实例——求平均值相关知识求平均数是MapReduce比较常见的算法，求平均数的算法也比较简单，一种思路是Map端读取数据，在数据输入到Reduce之前先经过shuffle，将map函数输出的key值相同的所有的value值形成一个集合value-list，然后将输入到Reduce端，Reduce端汇总并且统计记录数，然后作商即可。具体原理如下图所示： 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788//Mapper代码public static class Map extends Mapper&lt;Object , Text , Text , IntWritable&gt;{ private static Text newKey=new Text(); //实现map函数 public void map(Object key,Text value,Context context) throws IOException, InterruptedException{ // 将输入的纯文本文件的数据转化成String String line=value.toString(); System.out.println(line); String arr[]=line.split(\"\\t\"); newKey.set(arr[0]); int click=Integer.parseInt(arr[1]); context.write(newKey, new IntWritable(click)); } } //map端在采用Hadoop的默认输入方式之后，将输入的value值通过split()方法截取出来，我们把截取的商品点击次数字段转化为IntWritable类型并将其设置为value，把商品分类字段设置为key,然后直接输出key/value的值。//Reducer代码public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ //实现reduce函数 public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException{ int num=0; int count=0; for(IntWritable val:values){ num+=val.get(); //每个元素求和num count++; //统计元素的次数count } int avg=num/count; //计算平均数 context.write(key,new IntWritable(avg)); } } package mapreduce; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; public class MyAverage{ public static class Map extends Mapper&lt;Object , Text , Text , IntWritable&gt;{ private static Text newKey=new Text(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException{ String line=value.toString(); System.out.println(line); String arr[]=line.split(\"\\t\"); newKey.set(arr[0]); int click=Integer.parseInt(arr[1]); context.write(newKey, new IntWritable(click)); } } public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException{ int num=0; int count=0; for(IntWritable val:values){ num+=val.get(); count++; } int avg=num/count; context.write(key,new IntWritable(avg)); } } public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException{ Configuration conf=new Configuration(); System.out.println(\"start\"); Job job =new Job(conf,\"MyAverage\"); job.setJarByClass(MyAverage.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); Path in=new Path(\"hdfs://localhost:9000/mymapreduce4/in/goods_click\"); Path out=new Path(\"hdfs://localhost:9000/mymapreduce4/out\"); FileInputFormat.addInputPath(job,in); FileOutputFormat.setOutputPath(job,out); System.exit(job.waitForCompletion(true) ? 0 : 1); } } MapReduce实例——Reduce端join相关知识在Reudce端进行Join连接是MapReduce框架进行表之间Join操作最为常见的模式。 1.Reduce端Join实现原理 （1）Map端的主要工作，为来自不同表（文件）的key/value对打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。 （2）Reduce端的主要工作，在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在map阶段已经打标志）分开，最后进行笛卡尔只就ok了。 2.Reduce端Join的使用场景 Reduce端连接比Map端连接更为普遍，因为在map阶段不能获取所有需要的join字段，即：同一个key对应的字段可能位于不同map中，但是Reduce端连接效率比较低，因为所有数据都必须经过Shuffle过程。 3.本实验的Reduce端Join代码执行流程： （1）Map端读取所有的文件，并在输出的内容里加上标识，代表数据是从哪个文件里来的。 （2）在Reduce处理函数中，按照标识对数据进行处理。 （3）然后将相同的key值进行Join连接操作，求出结果并直接输出。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889//1)Map端读取所有的文件，并在输出的内容里加上标识，代表数据是从哪个文件里来的。//(2)在reduce处理函数中，按照标识对数据进行处理。//(3)然后将相同key值进行join连接操作，求出结果并直接输出。package mapreduce; import java.io.IOException; import java.util.Iterator; import java.util.Vector; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.FileSplit; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class ReduceJoin { public static class mymapper extends Mapper&lt;Object, Text, Text, Text&gt;{ @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException { String filePath = ((FileSplit)context.getInputSplit()).getPath().toString(); if (filePath.contains(\"orders1\")) { String line = value.toString(); String[] arr = line.split(\"\\t\"); context.write(new Text(arr[0]), new Text( \"1+\" + arr[2]+\"\\t\"+arr[3])); //System.out.println(arr[0] + \"_1+\" + arr[2]+\"\\t\"+arr[3]); }else if(filePath.contains(\"order_items1\")) { String line = value.toString(); String[] arr = line.split(\"\\t\"); context.write(new Text(arr[1]), new Text(\"2+\" + arr[2])); //System.out.println(arr[1] + \"_2+\" + arr[2]); } } } public static class myreducer extends Reducer&lt;Text, Text, Text, Text&gt;{ @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException { Vector&lt;String&gt; left = new Vector&lt;String&gt;(); Vector&lt;String&gt; right = new Vector&lt;String&gt;(); for (Text val : values) { String str = val.toString(); if (str.startsWith(\"1+\")) { left.add(str.substring(2)); } else if (str.startsWith(\"2+\")) { right.add(str.substring(2)); } } int sizeL = left.size(); int sizeR = right.size(); //System.out.println(key + \"left:\"+left); //System.out.println(key + \"right:\"+right); for (int i = 0; i &lt; sizeL; i++) { for (int j = 0; j &lt; sizeR; j++) { context.write( key, new Text( left.get(i) + \"\\t\" + right.get(j) ) ); //System.out.println(key + \" \\t\" + left.get(i) + \"\\t\" + right.get(j)); } } } } public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { Job job = Job.getInstance(); job.setJobName(\"reducejoin\"); job.setJarByClass(ReduceJoin.class); job.setMapperClass(mymapper.class); job.setReducerClass(myreducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); Path left = new Path(\"hdfs://localhost:9000/mymapreduce6/in/orders1\"); Path right = new Path(\"hdfs://localhost:9000/mymapreduce6/in/order_items1\"); Path out = new Path(\"hdfs://localhost:9000/mymapreduce6/out\"); FileInputFormat.addInputPath(job, left); FileInputFormat.addInputPath(job, right); FileOutputFormat.setOutputPath(job, out); System.exit(job.waitForCompletion(true) ? 0 : 1); } } MapReduce实例——Map端join相关知识MapReduce提供了表连接操作其中包括Map端join、Reduce端join还有单表连接，现在我们要讨论的是Map端join，Map端join是指数据到达map处理函数之前进行合并的，效率要远远高于Reduce端join，因为Reduce端join是把所有的数据都经过Shuffle，非常消耗资源。 1.Map端join的使用场景：一张表数据十分小、一张表数据很大。 Map端join是针对以上场景进行的优化：将小表中的数据全部加载到内存，按关键字建立索引。大表中的数据作为map的输入，对map()函数每一对&lt;key,value&gt;输入，都能够方便地和已加载到内存的小数据进行连接。把连接结果按key输出，经过shuffle阶段，reduce端得到的就是已经按key分组并且连接好了的数据。 为了支持文件的复制，Hadoop提供了一个类DistributedCache，使用该类的方法如下： （1）用户使用静态方法DistributedCache.addCacheFile()指定要复制的文件，它的参数是文件的URI（如果是HDFS上的文件，可以这样：hdfs://namenode:9000/home/XXX/file，其中9000是自己配置的NameNode端口号）。JobTracker在作业启动之前会获取这个URI列表，并将相应的文件拷贝到各个TaskTracker的本地磁盘上。 （2）用户使用DistributedCache.getLocalCacheFiles()方法获取文件目录，并使用标准的文件读写API读取相应的文件。 2.本实验Map端Join的执行流程 （1）首先在提交作业的时候先将小表文件放到该作业的DistributedCache中，然后从DistributeCache中取出该小表进行join连接的 &lt;key ,value&gt;键值对，将其解释分割放到内存中（可以放大Hash Map等等容器中）。 （2）要重写MyMapper类下面的setup()方法，因为这个方法是先于map方法执行的，将较小表先读入到一个HashMap中。 （3）重写map函数，一行行读入大表的内容，逐一的与HashMap中的内容进行比较，若Key相同，则对数据进行格式化处理，然后直接输出。 （4）map函数输出的&lt;key,value &gt;键值对首先经过一个suffle把key值相同的所有value放到一个迭代器中形成values，然后将&lt;key,values&gt;键值对传递给reduce函数，reduce函数输入的key直接复制给输出的key，输入的values通过增强版for循环遍历逐一输出，循环的次数决定了&lt;key,value&gt;输出的次数。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package mapreduce; import java.io.BufferedReader; import java.io.FileReader; import java.io.IOException; import java.net.URI; import java.net.URISyntaxException; import java.util.HashMap; import java.util.Map; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class MapJoin { public static class MyMapper extends Mapper&lt;Object, Text, Text, Text&gt;{ private Map&lt;String, String&gt; dict = new HashMap&lt;&gt;(); @Override protected void setup(Context context) throws IOException, InterruptedException { String fileName = context.getLocalCacheFiles()[0].getName(); //System.out.println(fileName); BufferedReader reader = new BufferedReader(new FileReader(fileName)); String codeandname = null; while (null != ( codeandname = reader.readLine() ) ) { String str[]=codeandname.split(\"\\t\"); dict.put(str[0], str[2]+\"\\t\"+str[3]); } reader.close(); } @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException { String[] kv = value.toString().split(\"\\t\"); if (dict.containsKey(kv[1])) { context.write(new Text(kv[1]), new Text(dict.get(kv[1])+\"\\t\"+kv[2])); } } } public static class MyReducer extends Reducer&lt;Text, Text, Text, Text&gt;{ @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException { for (Text text : values) { context.write(key, text); } } } public static void main(String[] args) throws ClassNotFoundException, IOException, InterruptedException, URISyntaxException { Job job = Job.getInstance(); job.setJobName(\"mapjoin\"); job.setJarByClass(MapJoin.class); job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); Path in = new Path(\"hdfs://localhost:9000/mymapreduce5/in/order_items1\"); Path out = new Path(\"hdfs://localhost:9000/mymapreduce5/out\"); FileInputFormat.addInputPath(job, in); FileOutputFormat.setOutputPath(job, out); URI uri = new URI(\"hdfs://localhost:9000/mymapreduce5/in/orders1\"); job.addCacheFile(uri); System.exit(job.waitForCompletion(true) ? 0 : 1); } } MapReduce实例——单表Join相关知识以本实验的buyer1(buyer_id,friends_id)表为例来阐述单表连接的实验原理。单表连接，连接的是左表的buyer_id列和右表的friends_id列，且左表和右表是同一个表。因此，在map阶段将读入数据分割成buyer_id和friends_id之后，会将buyer_id设置成key，friends_id设置成value，直接输出并将其作为左表；再将同一对buyer_id和friends_id中的friends_id设置成key，buyer_id设置成value进行输出，作为右表。为了区分输出中的左右表，需要在输出的value中再加上左右表的信息，比如在value的String最开始处加上字符1表示左表，加上字符2表示右表。这样在map的结果中就形成了左表和右表，然后在shuffle过程中完成连接。reduce接收到连接的结果，其中每个key的value-list就包含了”buyer_idfriends_id–friends_idbuyer_id”关系。取出每个key的value-list进行解析，将左表中的buyer_id放入一个数组，右表中的friends_id放入一个数组，然后对两个数组求笛卡尔积就是最后的结果了。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package mapreduce; import java.io.IOException; import java.util.Iterator; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class DanJoin { public static class Map extends Mapper&lt;Object,Text,Text,Text&gt;{ public void map(Object key,Text value,Context context) throws IOException,InterruptedException{ String line = value.toString(); String[] arr = line.split(\"\\t\"); String mapkey=arr[0]; String mapvalue=arr[1]; String relationtype=new String(); relationtype=\"1\"; context.write(new Text(mapkey),new Text(relationtype+\"+\"+mapvalue)); //System.out.println(relationtype+\"+\"+mapvalue); relationtype=\"2\"; context.write(new Text(mapvalue),new Text(relationtype+\"+\"+mapkey)); //System.out.println(relationtype+\"+\"+mapvalue); } } public static class Reduce extends Reducer&lt;Text, Text, Text, Text&gt;{ public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException,InterruptedException{ int buyernum=0; String[] buyer=new String[20]; int friendsnum=0; String[] friends=new String[20]; Iterator ite=values.iterator(); while(ite.hasNext()){ String record=ite.next().toString(); int len=record.length(); int i=2; if(0==len){ continue; } char relationtype=record.charAt(0); if('1'==relationtype){ buyer [buyernum]=record.substring(i); buyernum++; } if('2'==relationtype){ friends[friendsnum]=record.substring(i); friendsnum++; } } if(0!=buyernum&amp;&amp;0!=friendsnum){ for(int m=0;m&lt;buyernum;m++){ for(int n=0;n&lt;friendsnum;n++){ if(buyer[m]!=friends[n]){ context.write(new Text(buyer[m]),new Text(friends[n])); } } } } } } public static void main(String[] args) throws Exception{ Configuration conf=new Configuration(); String[] otherArgs=new String[2]; otherArgs[0]=\"hdfs://localhost:9000/mymapreduce7/in/buyer1\"; otherArgs[1]=\"hdfs://localhost:9000/mymapreduce7/out\"; Job job=new Job(conf,\" Table join\"); job.setJarByClass(DanJoin.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true)?0:1); } } MapReduce实例——二次排序相关知识在Map阶段，使用job.setInputFormatClass定义的InputFormat将输入的数据集分割成小数据块splites，同时InputFormat提供一个RecordReder的实现。本实验中使用的是TextInputFormat，他提供的RecordReder会将文本的字节偏移量作为key，这一行的文本作为value。这就是自定义Map的输入是&lt;LongWritable, Text&gt;的原因。然后调用自定义Map的map方法，将一个个&lt;LongWritable, Text&gt;键值对输入给Map的map方法。注意输出应该符合自定义Map中定义的输出&lt;IntPair, IntWritable&gt;。最终是生成一个List&lt;IntPair, IntWritable&gt;。在map阶段的最后，会先调用job.setPartitionerClass对这个List进行分区，每个分区映射到一个reducer。每个分区内又调用job.setSortComparatorClass设置的key比较函数类排序。可以看到，这本身就是一个二次排序。 如果没有通过job.setSortComparatorClass设置key比较函数类，则可以使用key实现的compareTo方法进行排序。 在本实验中，就使用了IntPair实现的compareTo方法。 在Reduce阶段，reducer接收到所有映射到这个reducer的map输出后，也是会调用job.setSortComparatorClass设置的key比较函数类对所有数据对排序。然后开始构造一个key对应的value迭代器。这时就要用到分组，使用job.setGroupingComparatorClass设置的分组函数类。只要这个比较器比较的两个key相同，他们就属于同一个组，它们的value放在一个value迭代器，而这个迭代器的key使用属于同一个组的所有key的第一个key。最后就是进入Reducer的reduce方法，reduce方法的输入是所有的（key和它的value迭代器）。同样注意输入与输出的类型必须与自定义的Reducer中声明的一致。 code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196package mapreduce; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.io.WritableComparable; import org.apache.hadoop.io.WritableComparator; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Partitioner; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; public class SecondarySort { public static class IntPair implements WritableComparable&lt;IntPair&gt; { int first; int second; public void set(int left, int right) { first = left; second = right; } public int getFirst() { return first; } public int getSecond() { return second; } @Override public void readFields(DataInput in) throws IOException { // TODO Auto-generated method stub first = in.readInt(); second = in.readInt(); } @Override public void write(DataOutput out) throws IOException { // TODO Auto-generated method stub out.writeInt(first); out.writeInt(second); } @Override public int compareTo(IntPair o) { // TODO Auto-generated method stub if (first != o.first) { return first &lt; o.first ? 1 : -1; } else if (second != o.second) { return second &lt; o.second ? -1 : 1; } else { return 0; } } @Override public int hashCode() { return first * 157 + second; } @Override public boolean equals(Object right) { if (right == null) return false; if (this == right) return true; if (right instanceof IntPair) { IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; } else { return false; } } } public static class FirstPartitioner extends Partitioner&lt;IntPair, IntWritable&gt; { @Override public int getPartition(IntPair key, IntWritable value,int numPartitions) { return Math.abs(key.getFirst() * 127) % numPartitions; } } public static class GroupingComparator extends WritableComparator { protected GroupingComparator() { super(IntPair.class, true); } @Override //Compare two WritableComparables. public int compare(WritableComparable w1, WritableComparable w2) { IntPair ip1 = (IntPair) w1; IntPair ip2 = (IntPair) w2; int l = ip1.getFirst(); int r = ip2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); } } public static class Map extends Mapper&lt;LongWritable, Text, IntPair, IntWritable&gt; { private final IntPair intkey = new IntPair(); private final IntWritable intvalue = new IntWritable(); public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); StringTokenizer tokenizer = new StringTokenizer(line); int left = 0; int right = 0; if (tokenizer.hasMoreTokens()) { left = Integer.parseInt(tokenizer.nextToken()); if (tokenizer.hasMoreTokens()) right = Integer.parseInt(tokenizer.nextToken()); intkey.set(right, left); intvalue.set(left); context.write(intkey, intvalue); } } } public static class Reduce extends Reducer&lt;IntPair, IntWritable, Text, IntWritable&gt; { private final Text left = new Text(); private static final Text SEPARATOR = new Text(\"------------------------------------------------\"); public void reduce(IntPair key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException { context.write(SEPARATOR, null); left.set(Integer.toString(key.getFirst())); System.out.println(left); for (IntWritable val : values) { context.write(left, val); //System.out.println(val); } } } public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException { Configuration conf = new Configuration(); Job job = new Job(conf, \"secondarysort\"); job.setJarByClass(SecondarySort.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setPartitionerClass(FirstPartitioner.class); job.setGroupingComparatorClass(GroupingComparator.class); job.setMapOutputKeyClass(IntPair.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); String[] otherArgs=new String[2]; otherArgs[0]=\"hdfs://localhost:9000/mymapreduce8/in/goods_visit2\"; otherArgs[1]=\"hdfs://localhost:9000/mymapreduce8/out\"; FileInputFormat.setInputPaths(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } MapReduce实例——倒排索引相关知识“倒排索引”是文档检索系统中最常用的数据结构，被广泛地应用于全文搜索引擎。它主要是用来存储某个单词（或词组）在一个文档或一组文档中的存储位置的映射，即提供了一种根据内容来查找文档的方式。由于不是根据文档来确定文档所包含的内容，而是进行相反的操作，因而称为倒排索引（Inverted Index）。 实现”倒排索引”主要关注的信息为：单词、文档URL及词频。 下面以本实验goods3、goods_visit3、order_items3三张表的数据为例，根据MapReduce的处理过程给出倒排索引的设计思路： （1）Map过程 首先使用默认的TextInputFormat类对输入文件进行处理，得到文本中每行的偏移量及其内容。显然，Map过程首先必须分析输入的&lt;key,value&gt;对，得到倒排索引中需要的三个信息：单词、文档URL和词频，接着我们对读入的数据利用Map操作进行预处理，如下图所示： 这里存在两个问题：第一，&lt;key,value&gt;对只能有两个值，在不使用Hadoop自定义数据类型的情况下，需要根据情况将其中两个值合并成一个值，作为key或value值。第二，通过一个Reduce过程无法同时完成词频统计和生成文档列表，所以必须增加一个Combine过程完成词频统计。 这里将商品ID和URL组成key值（如”1024600：goods3”），将词频（商品ID出现次数）作为value，这样做的好处是可以利用MapReduce框架自带的Map端排序，将同一文档的相同单词的词频组成列表，传递给Combine过程，实现类似于WordCount的功能。 （2）Combine过程 经过map方法处理后，Combine过程将key值相同的value值累加，得到一个单词在文档中的词频，如下图所示。如果直接将下图所示的输出作为Reduce过程的输入，在Shuffle过程时将面临一个问题：所有具有相同单词的记录（由单词、URL和词频组成）应该交由同一个Reducer处理，但当前的key值无法保证这一点，所以必须修改key值和value值。这次将单词（商品ID）作为key值，URL和词频组成value值（如”goods3：1”）。这样做的好处是可以利用MapReduce框架默认的HashPartitioner类完成Shuffle过程，将相同单词的所有记录发送给同一个Reducer进行处理。 （3）Reduce过程 经过上述两个过程后，Reduce过程只需将相同key值的所有value值组合成倒排索引文件所需的格式即可，剩下的事情就可以直接交给MapReduce框架进行处理了。如下图所示 code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596package mapreduce; import java.io.IOException; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.FileSplit; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class MyIndex { public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { Job job = Job.getInstance(); job.setJobName(\"InversedIndexTest\"); job.setJarByClass(MyIndex.class); job.setMapperClass(doMapper.class); job.setCombinerClass(doCombiner.class); job.setReducerClass(doReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); Path in1 = new Path(\"hdfs://localhost:9000/mymapreduce9/in/goods3\"); Path in2 = new Path(\"hdfs://localhost:9000/mymapreduce9/in/goods_visit3\"); Path in3 = new Path(\"hdfs://localhost:9000/mymapreduce9/in/order_items3\"); Path out = new Path(\"hdfs://localhost:9000/mymapreduce9/out\"); FileInputFormat.addInputPath(job, in1); FileInputFormat.addInputPath(job, in2); FileInputFormat.addInputPath(job, in3); FileOutputFormat.setOutputPath(job, out); System.exit(job.waitForCompletion(true) ? 0 : 1); } public static class doMapper extends Mapper&lt;Object, Text, Text, Text&gt;{ public static Text myKey = new Text(); public static Text myValue = new Text(); //private FileSplit filePath; @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException { String filePath=((FileSplit)context.getInputSplit()).getPath().toString(); if(filePath.contains(\"goods\")){ String val[]=value.toString().split(\"\\t\"); int splitIndex =filePath.indexOf(\"goods\"); myKey.set(val[0] + \":\" + filePath.substring(splitIndex)); }else if(filePath.contains(\"order\")){ String val[]=value.toString().split(\"\\t\"); int splitIndex =filePath.indexOf(\"order\"); myKey.set(val[2] + \":\" + filePath.substring(splitIndex)); } myValue.set(\"1\"); context.write(myKey, myValue); } } public static class doCombiner extends Reducer&lt;Text, Text, Text, Text&gt;{ public static Text myK = new Text(); public static Text myV = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException { int sum = 0 ; for (Text value : values) { sum += Integer.parseInt(value.toString()); } int mysplit = key.toString().indexOf(\":\"); myK.set(key.toString().substring(0, mysplit)); myV.set(key.toString().substring(mysplit + 1) + \":\" + sum); context.write(myK, myV); } } public static class doReducer extends Reducer&lt;Text, Text, Text, Text&gt;{ public static Text myK = new Text(); public static Text myV = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException { String myList = new String(); for (Text value : values) { myList += value.toString() + \";\"; } myK.set(key); myV.set(myList); context.write(myK, myV); } } } MapReduce实例——ChainMapReduce相关知识一些复杂的任务难以用一次MapReduce处理完成，需要多次MapReduce才能完成任务。Hadoop2.0开始MapReduce作业支持链式处理，类似于工厂的生产线，每一个阶段都有特定的任务要处理，比如提供原配件——&gt;组装——打印出厂日期，等等。通过这样进一步的分工，从而提高了生成效率，我们Hadoop中的链式MapReduce也是如此，这些Mapper可以像水流一样，一级一级向后处理，有点类似于Linux的管道。前一个Mapper的输出结果直接可以作为下一个Mapper的输入，形成一个流水线。 链式MapReduce的执行规则：整个Job中只能有一个Reducer，在Reducer前面可以有一个或者多个Mapper，在Reducer的后面可以有0个或者多个Mapper。 Hadoop2.0支持的链式处理MapReduce作业有以下三种： （1）顺序链接MapReduce作业 类似于Unix中的管道：mapreduce-1 | mapreduce-2 | mapreduce-3 ……，每一个阶段创建一个job，并将当前输入路径设为前一个的输出。在最后阶段删除链上生成的中间数据。 （2）具有复杂依赖的MapReduce链接 若mapreduce-1处理一个数据集， mapreduce-2 处理另一个数据集，而mapreduce-3对前两个做内部链接。这种情况通过Job和JobControl类管理非线性作业间的依赖。如x.addDependingJob(y)意味着x在y完成前不会启动。 （3）预处理和后处理的链接 一般将预处理和后处理写为Mapper任务。可以自己进行链接或使用ChainMapper和ChainReducer类，生成作业表达式类似于： MAP+ | REDUCE | MAP* 如以下作业： Map1 | Map2 | Reduce | Map3 | Map4，把Map2和Reduce视为MapReduce作业核心。Map1作为前处理，Map3， Map4作为后处理。ChainMapper使用模式：预处理作业，ChainReducer使用模式：设置Reducer并添加后处理Mapper 本实验中用到的就是第三种作业模式：预处理和后处理的链接，生成作业表达式类似于 Map1 | Map2 | Reduce | Map3 code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package mapreduce; import java.io.IOException; import java.net.URI; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.chain.ChainMapper; import org.apache.hadoop.mapreduce.lib.chain.ChainReducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.io.DoubleWritable; public class ChainMapReduce { private static final String INPUTPATH = \"hdfs://localhost:9000/mymapreduce10/in/goods_0\"; private static final String OUTPUTPATH = \"hdfs://localhost:9000/mymapreduce10/out\"; public static void main(String[] args) { try { Configuration conf = new Configuration(); FileSystem fileSystem = FileSystem.get(new URI(OUTPUTPATH), conf); if (fileSystem.exists(new Path(OUTPUTPATH))) { fileSystem.delete(new Path(OUTPUTPATH), true); } Job job = new Job(conf, ChainMapReduce.class.getSimpleName()); FileInputFormat.addInputPath(job, new Path(INPUTPATH)); job.setInputFormatClass(TextInputFormat.class); ChainMapper.addMapper(job, FilterMapper1.class, LongWritable.class, Text.class, Text.class, DoubleWritable.class, conf); ChainMapper.addMapper(job, FilterMapper2.class, Text.class, DoubleWritable.class, Text.class, DoubleWritable.class, conf); ChainReducer.setReducer(job, SumReducer.class, Text.class, DoubleWritable.class, Text.class, DoubleWritable.class, conf); ChainReducer.addMapper(job, FilterMapper3.class, Text.class, DoubleWritable.class, Text.class, DoubleWritable.class, conf); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(DoubleWritable.class); job.setPartitionerClass(HashPartitioner.class); job.setNumReduceTasks(1); job.setOutputKeyClass(Text.class); job.setOutputValueClass(DoubleWritable.class); FileOutputFormat.setOutputPath(job, new Path(OUTPUTPATH)); job.setOutputFormatClass(TextOutputFormat.class); System.exit(job.waitForCompletion(true) ? 0 : 1); } catch (Exception e) { e.printStackTrace(); } } public static class FilterMapper1 extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt; { private Text outKey = new Text(); private DoubleWritable outValue = new DoubleWritable(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;.Context context) throws IOException,InterruptedException { String line = value.toString(); if (line.length() &gt; 0) { String[] splits = line.split(\"\\t\"); double visit = Double.parseDouble(splits[1].trim()); if (visit &lt;= 600) { outKey.set(splits[0]); outValue.set(visit); context.write(outKey, outValue); } } } } public static class FilterMapper2 extends Mapper&lt;Text, DoubleWritable, Text, DoubleWritable&gt; { @Override protected void map(Text key, DoubleWritable value, Mapper&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context) throws IOException,InterruptedException { if (value.get() &lt; 100) { context.write(key, value); } } } public static class SumReducer extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt; { private DoubleWritable outValue = new DoubleWritable(); @Override protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values, Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context) throws IOException, InterruptedException { double sum = 0; for (DoubleWritable val : values) { sum += val.get(); } outValue.set(sum); context.write(key, outValue); } } public static class FilterMapper3 extends Mapper&lt;Text, DoubleWritable, Text, DoubleWritable&gt; { @Override protected void map(Text key, DoubleWritable value, Mapper&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context) throws IOException, InterruptedException { if (key.toString().length() &lt; 3) { System.out.println(\"写出去的内容为：\" + key.toString() +\"++++\"+ value.toString()); context.write(key, value); } } } } MapReduce实战PageRank算法相关知识PageRank：网页排名，右脚网页级别。是以Google 公司创始人Larry Page 之姓来命名。PageRank 计算每一个网页的PageRank值，并根据PageRank值的大小对网页的重要性进行排序。 PageRank的基本思想： 1.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高 2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高。 PageRank的算法原理： PageRank算法总的来说就是预先给每个网页一个PR值，由于PR值物理意义上为一个网页被访问概率，所以一般是1/N，其中N为网页总数。另外，一般情况下，所有网页的PR值的总和为1。如果不为1的话也不是不行，最后算出来的不同网页之间PR值的大小关系仍然是正确的，只是不能直接地反映概率了。 如图，假设现在有四张网页，对于页面A来说，它链接到页面B，C，D，即A有3个出链，则它跳转到每个出链B，C，D的概率均为1/3.。如果A有k个出链，跳转到每个出链的概率为1/k。同理B到A，C，D的概率为1/2，0，1/2。C到A，B，D的概率为1，0，0。D到A，B，C的概率为0，1/2，1/2。 转化为矩阵为： 在上图中，第一列为页面A对各个页面转移的概率，第一行为各个页面对页面A转移的概率。初始时，每一个页面的PageRank值都是均等的，为1/N，这里也即是1/4。然后对于页面A来说，根据每一个页面的PageRank值和每个页面对页面A的转移概率，可以算出新一轮页面A的PageRank值。这里，只有页面B转移了自己的1/2给A。页面C转移了自己的全部给A，所以新一轮A的PageRank值为1/41/2+1/41=9/24。 为了计算方便，我们设置各页面初始的PageRank值为一个列向量V0。然后再基于转移矩阵，我们可以直接求出新一轮各个页面的PageRank值。即 V1 = MV0 现在得到了各页面新的PageRank值V1, 继续用M 去乘以V1 ,就会得到更新的PageRank值。一直迭代这个过程，可以证明出V最终会收敛。此时停止迭代。这时的V就是各个页面的PageRank值。 处理Dead Ends(终止点)： 上面的PageRank计算方法要求整个Web是强联通的。而实际上真实的Web并不是强联通的，有一类页面，它们不存在任何外链，对其他网页没有PR值的贡献，称之为Dead Ends(终止点)。如下图： 这里页面C即是一个终止点。而上面的算法之所以能够成功收敛，很大因素上基于转移矩阵每一列的和为1（每一个页面都至少有一个出链）。当页面C没有出链时，转移矩阵M如下所示： 基于这个转移矩阵和初始的PageRank列向量，每一次迭代过的PageRank列向量如下： 解决该问题的一种方法是：迭代拿掉图中的Dead Ends点以及相关的边，之所以是迭代拿掉，是因为当拿掉最初的Dead Ends之后，又可能产生新的Dead Ends点。直到图中没有Dead Ends点为止。然后对剩余的所有节点，计算它们的PageRank ，然后以拿掉Dead Ends的逆序反推各个Dead Ends的PageRank值。 比如在上图中，首先拿掉页面C，发现没有产生新的Dead Ends。然后对A，B，D 计算他们的PageRank，他们初始PageRank值均为1/3，且A有两个出链，B有两个出链，D有一个出链，那么由上面的方法可以算出各页面最终的PageRank值。假设算出A的PageRank 为x，B的PageRank 为y，D的PageRank 为z，那么C的PageRank值为1/3x + 1/2z 。 处理Spider Traps（蜘蛛陷阱）： 真实的Web链接关系若是转换成转移矩阵，那必将是一个稀疏的矩阵。而稀疏的矩阵迭代相乘会使得中间产生的PageRank向量变得不平滑（一小部分值很大，大部分值很小或接近于0）。而一种Spider Traps节点会加剧这个不平滑的效果，也即是蜘蛛陷阱。它是指某一些页面虽然有外链，但是它只链向自己。如下图所示： 如果对这个图按照上面的方法进行迭代计算PageRank ， 计算后会发现所有页面的PageRank值都会逐步转移到页面C上来，而其他页面都趋近于零。 为了解决这个问题，我们需要对PageRank 计算方法进行一个平滑处理–加入teleporting(跳转因子)。也就是说，用户在访问Web页面时，除了按照Web页面的链接关系进行选择以外，他也可能直接在地址栏上输入一个地址进行访问。这样就避免了用户只在一个页面只能进行自身访问，或者进入一个页面无法出来的情况。 加入跳转因子之后，PageRank向量的计算公式修正为： 其中，β 通常设置为一个很小的数（0.2或者0.15），e为单位向量，N是所有页面的个数，乘以1/N是因为随机跳转到一个页面的概率是1/N。这样，每次计算PageRank值，既依赖于转移矩阵，同时依赖于小概率的随机跳转。 以上图为例，改进后的PageRank值计算如下： 按照这个计算公式迭代下去，会发现spider traps 效应被抑制了，使得各个页面得到一个合理的PageRank值。 code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package mr_pagerank; import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class PageRank { /*map过程*/ public static class mapper extends Mapper&lt;Object,Text,Text,Text&gt;{ private String id; private float pr; private int count; private float average_pr; public void map(Object key,Text value,Context context) throws IOException,InterruptedException{ StringTokenizer str = new StringTokenizer(value.toString());//对value进行解析 id =str.nextToken();//id为解析的第一个词，代表当前网页 pr = Float.parseFloat(str.nextToken());//pr为解析的第二个词，转换为float类型，代表PageRank值 count = str.countTokens();//count为剩余词的个数，代表当前网页的出链网页个数 average_pr = pr/count;//求出当前网页对出链网页的贡献值 String linkids =\"&amp;\";//下面是输出的两类，分别有'@'和'&amp;'区分 while(str.hasMoreTokens()){ String linkid = str.nextToken(); context.write(new Text(linkid),new Text(\"@\"+average_pr));//输出的是&lt;出链网页，获得的贡献值&gt; linkids +=\" \"+ linkid; } context.write(new Text(id), new Text(linkids));//输出的是&lt;当前网页，所有出链网页&gt; } } /*reduce过程*/ public static class reduce extends Reducer&lt;Text,Text,Text,Text&gt;{ public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException,InterruptedException{ String link = \"\"; float pr = 0; /*对values中的每一个value进行分析，通过其第一个字符是'@'还是'&amp;'进行判断 通过这个循环，可以求出当前网页获得的贡献值之和，也即是新的PageRank值；同时求出当前 网页的所有出链网页 */ for(Text val:values){ if(val.toString().substring(0,1).equals(\"@\")){ pr += Float.parseFloat(val.toString().substring(1)); } else if(val.toString().substring(0,1).equals(\"&amp;\")){ link += val.toString().substring(1); } } pr = 0.8f*pr + 0.2f*0.25f;//加入跳转因子，进行平滑处理 String result = pr+link; context.write(key, new Text(result)); } } public static void main(String[] args) throws Exception{ Configuration conf = new Configuration(); conf.set(\"mapred.job.tracker\", \"hdfs://127.0.0.1:9000\"); //设置数据输入路径 String pathIn =\"hdfs://127.0.0.1:9000/pagerank/input\"; //设置数据输出路径 String pathOut=\"hdfs://127.0.0.1:9000/pagerank/output/pr\"; for(int i=1;i&lt;100;i++){ //加入for循环，最大循环100次 Job job = new Job(conf,\"page rank\"); job.setJarByClass(PageRank.class); job.setMapperClass(mapper.class); job.setReducerClass(reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job, new Path(pathIn)); FileOutputFormat.setOutputPath(job, new Path(pathOut)); pathIn = pathOut;//把输出的地址改成下一次迭代的输入地址 pathOut = pathOut+'-'+i;//把下一次的输出设置成一个新地址。 System.out.println(\"正在执行第\"+i+\"次\"); job.waitForCompletion(true);//把System.exit()去掉 //由于PageRank通常迭代30~40次，就可以收敛，这里我们设置循环35次 if(i == 35){ System.out.println(\"总共执行了\"+i+\"次之后收敛\"); break; } } } }","link":"/2020/03/26/BigData/Hadoop%E2%80%94%E2%80%94MapReduce%E5%AD%A6%E4%B9%A0/"},{"title":"05-09-2020 1994 Text1","text":"","link":"/2020/05/10/study%20english/05-09-2020-1994-Text1/"},{"title":"05-10-2020 1994 Text2","text":"","link":"/2020/05/10/study%20english/05-10-2020-1994-Text2/"}],"tags":[{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"喜爱的图片","slug":"喜爱的图片","link":"/tags/%E5%96%9C%E7%88%B1%E7%9A%84%E5%9B%BE%E7%89%87/"},{"name":"Linux命令","slug":"Linux命令","link":"/tags/Linux%E5%91%BD%E4%BB%A4/"},{"name":"NLP(自然语言处理)","slug":"NLP-自然语言处理","link":"/tags/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"MyBooks","slug":"MyBooks","link":"/tags/MyBooks/"},{"name":"MySQL Server","slug":"MySQL-Server","link":"/tags/MySQL-Server/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"Daily Sentence","slug":"Daily-Sentence","link":"/tags/Daily-Sentence/"},{"name":"Daily Reading","slug":"Daily-Reading","link":"/tags/Daily-Reading/"}],"categories":[{"name":"大数据技术","slug":"大数据技术","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"Life","slug":"Life","link":"/categories/Life/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"NLP(自然语言处理)","slug":"NLP-自然语言处理","link":"/categories/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"BOOKS","slug":"BOOKS","link":"/categories/BOOKS/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"Daily Sentence","slug":"Daily-Sentence","link":"/categories/Daily-Sentence/"},{"name":"Daily Reading","slug":"Daily-Reading","link":"/categories/Daily-Reading/"}]}