{"meta":{"title":"Shaw","subtitle":"","description":"","author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2020-03-09T06:41:09.602Z","updated":"2020-03-09T06:41:02.356Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-03-09T06:41:53.588Z","updated":"2020-03-09T06:41:48.230Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"参考文献","slug":"参考文献","date":"2020-03-15T08:48:42.000Z","updated":"2020-03-15T08:52:37.667Z","comments":true,"path":"2020/03/15/参考文献/","link":"","permalink":"http://yoursite.com/2020/03/15/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/","excerpt":"","text":"[音乐推荐系统主观评价之变研究–以网易云音乐为例](file:///F:/My_Study/Scientific Res/holiday have been finished/音乐推荐系统主观评价指标研究——以网易云音乐为例.pdf)","categories":[{"name":"科研参考文献","slug":"科研参考文献","permalink":"http://yoursite.com/categories/%E7%A7%91%E7%A0%94%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/"}],"tags":[{"name":"科研参考文献","slug":"科研参考文献","permalink":"http://yoursite.com/tags/%E7%A7%91%E7%A0%94%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/"}]},{"title":"2020-3-9 (From Criminal Minds)","slug":"study english/2020-3-9","date":"2020-03-09T14:27:56.928Z","updated":"2020-03-09T14:44:03.092Z","comments":true,"path":"2020/03/09/study english/2020-3-9/","link":"","permalink":"http://yoursite.com/2020/03/09/study%20english/2020-3-9/","excerpt":"","text":"Better to write for youself and have no public.Then to write for the public and have noself.","categories":[{"name":"Daily Sentence","slug":"Daily-Sentence","permalink":"http://yoursite.com/categories/Daily-Sentence/"}],"tags":[{"name":"Daily Sentence","slug":"Daily-Sentence","permalink":"http://yoursite.com/tags/Daily-Sentence/"}]},{"title":"基于维基百科中文语料库的Word2Vec模型训练","slug":"NLP/nlp1","date":"2020-03-09T07:42:53.479Z","updated":"2020-03-09T08:04:01.803Z","comments":true,"path":"2020/03/09/NLP/nlp1/","link":"","permalink":"http://yoursite.com/2020/03/09/NLP/nlp1/","excerpt":"说明：该博客代码参考于参考博客：使用中文维基百科语料库+opencc+jieba+gensim训练一个word2vec模型参考博客：使用中文维基百科训练word2vec模型 零、 模型训练环境 Windows10-X64 、 python2.7 、 python3.6 pip install jieba pip install gensim","text":"说明：该博客代码参考于参考博客：使用中文维基百科语料库+opencc+jieba+gensim训练一个word2vec模型参考博客：使用中文维基百科训练word2vec模型 零、 模型训练环境 Windows10-X64 、 python2.7 、 python3.6 pip install jieba pip install gensim 一、下载维基百科语料库数据下载地址该博客使用的是 24-Nov-2019 的语料库 二、 利用WikiExtractor.py抽取正文WikiExtractor.py Copy链接直接把py文件中的源码直接复制到新的 .py 中即可。下图是模型训练相关文件：执行命令 1python WiKiExtractor.py -b 500M -o zhwiki zhwiki-20191120-pages-articles-multistream.xml.bz2 我们最终会生成zhwiki文件夹得到三个文本文件 ./zhwiki/AA/wiki_00./zhwiki/AA/wiki_01./zhwiki/AA/wiki_02 三、 中文繁体转换成简体简体直接使用开源项目转换工具opencc转换工具下载地址黑框框起来的是我下载的版本 执行文件添加到环境变量中：方法1. 解压后将该工具bin目录添加到环境变量中方法2. 有些工具解压后灭有 bin 目录，我们首先找到opencc.exe可执行文件，将其路径添加到环境变量中 运行命令我是在该路径中运行的命令，为了方便我先将之前的三个文件拷贝到该路径下，执行完后再拷贝到./zhwiki/ 下。 123.\\opencc -i wiki_00 -o zh_wiki_00 -c /path/to/t2s.json.\\opencc -i wiki_01 -o zh_wiki_01 -c /path/to/t2s.json.\\opencc -i wiki_02 -o zh_wiki_02 -c /path/to/t2s.json 最终我把生成的zh_wiki_00等文件拷贝到 /zhwiki/ 中 四、符号处理 ，整合文件python 2.7 环境中运行exec_sum.py 1234567891011121314151617181920212223242526#!python2import reimport sysimport codecsdef myfun(input_file): p1 = re.compile(ur'-\\&#123;.*?(zh-hans|zh-cn):([^;]*?)(;.*?)?\\&#125;-') p2 = re.compile(ur'[（\\(][，；。？！\\s]*[）\\)]') p3 = re.compile(ur'[「『]') p4 = re.compile(ur'[」』]') outfile = codecs.open('std_zh_wiki', 'a+', 'utf-8') with codecs.open(input_file, 'r', 'utf-8') as myfile: for line in myfile: line = p1.sub(ur'\\2', line) line = p2.sub(ur'', line) line = p3.sub(ur'“', line) line = p4.sub(ur'”', line) outfile.write(line) outfile.close()if __name__ == '__main__': if len(sys.argv) != 2: print(\"Usage: python script.py inputfile\") sys.exit() reload(sys) sys.setdefaultencoding('utf-8') input_file = sys.argv[1] myfun(input_file) 方法一：将三个文件处理后追加到一个文件中即 std_zh_wiki 123python .\\exec_sum.py zh_wiki_00python .\\exec_sum.py zh_wiki_01python .\\exec_sum.py zh_wiki_02 方法二：我是使用 pycharm IDE 运行了 三次该文件，每次运行修改 input_file(zh_wiki_00、zh_wiki_01、zh_wiki_02) (更改python环境更方便) 五、jieba 分词操作exec_cut.py 123456789101112131415161718192021222324252627282930313233343536import logging, jieba, os, redef get_stopwords(): logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO) # 加载停用词表 stopword_set = set() with open(\"stopwords.txt\", 'r') as stopwords: for stopword in stopwords: stopword_set.add(stopword.strip(\"\\n\")) return stopword_setdef parse_zhwiki(read_file_path, save_file_path): # 过滤掉&lt;doc&gt; regex_str = \"[^&lt;doc.*&gt;$]|[^&lt;/doc&gt;$]\" file = open(read_file_path, \"r\") output = open(save_file_path, \"w+\") content_line = file.readline() stopwords = get_stopwords() article_contents = \"\" while content_line: match_obj = re.match(regex_str, content_line) content_line = content_line.strip(\"\\n\") if len(content_line) &gt; 0: if match_obj: words = jieba.cut(content_line, cut_all=False) for word in words: if word not in stopwords: article_contents += word + \" \" else: if len(article_contents) &gt; 0: output.write(article_contents.encode('utf-8','ignore')+ \"\\n\") article_contents = \"\" content_line = file.readline() output.close()parse_zhwiki('./std_zh_wiki', './cut_std_zh_wiki') 该操作也是在 python 2.7环境下操作 分词时间相对较长，请耐心等待 六、 训练Word2Vec模型1234567from gensim.models import word2vecimport logginglogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)sentences = word2vec.LineSentence('./cut_std_zh_wiki')model = word2vec.Word2Vec(sentences,size=200,window=5,min_count=5,workers=4)model.save('WikiCHModel') 训练完后，会生成三个文件 如果python3环境下出现编码问题，请通过某些工具将文件转换成utf-8编码 如果无法更改编码，请使用python2.7 七、 模型测试123456from gensim.models import word2vecimport loggingfrom gensim import modelsmodel = word2vec.Word2Vec.load('WikiCHModel')print(model.wv.similarity(\"儿童\", '狗')) #两个词的相关性 若博客中内容存在问题，可在评论处留言，本人将及时更正相关内容","categories":[{"name":"NLP(自然语言处理)","slug":"NLP-自然语言处理","permalink":"http://yoursite.com/categories/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"NLP(自然语言处理)","slug":"NLP-自然语言处理","permalink":"http://yoursite.com/tags/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}]},{"title":"解决NLTK语料库下载出错及nltk_data路径等问题","slug":"NLP/nlp_nltk","date":"2020-03-09T07:23:49.285Z","updated":"2020-03-09T07:26:00.996Z","comments":true,"path":"2020/03/09/NLP/nlp_nltk/","link":"","permalink":"http://yoursite.com/2020/03/09/NLP/nlp_nltk/","excerpt":"一、解决NLTK语料库下载问题NLTK有许多可供使用的语料库，但直接通过官网下载会出现某些问题下载语料库代码 12import nltknltk.download(\"all\") 代码中添加的参数可参考官方链接","text":"一、解决NLTK语料库下载问题NLTK有许多可供使用的语料库，但直接通过官网下载会出现某些问题下载语料库代码 12import nltknltk.download(\"all\") 代码中添加的参数可参考官方链接 问题一：下载速度过慢，考验你的耐心 问题二：下载速度过慢导致下载中止，再次运行命令会导致下图情况 解决方案：通过百度资源下载nltk_data链接：nltk_data提取码：ucun 二、nltk_data路径问题1.直接解压在C盘根目录下(C:\\nltk_data)测试代码 1234from nltk.book import *from nltk.corpus import reutersfiles = reuters.fileids()print(files) 2.放在任意目录下:测试代码 123456from nltk import datafrom nltk.corpus import reuters#每次访问数据需要添加数据至路径当中data.path.append(r\"F:\\About-Python\\NLP_env_1\\note\\nltk_data\")files = reuters.fileids()print(files) 输出结果遇见问题级解决方案持续更新中","categories":[{"name":"NLP(自然语言处理)","slug":"NLP-自然语言处理","permalink":"http://yoursite.com/categories/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"NLP(自然语言处理)","slug":"NLP-自然语言处理","permalink":"http://yoursite.com/tags/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}]},{"title":"python-math标准库的使用","slug":"python/python1","date":"2020-03-09T06:53:59.000Z","updated":"2020-03-09T07:20:42.215Z","comments":true,"path":"2020/03/09/python/python1/","link":"","permalink":"http://yoursite.com/2020/03/09/python/python1/","excerpt":"如何查看math标准库中的相关方法: 12import mathprint(dir(math)) 12345['__doc__', '__loader__', '__name__', '__package__', '__spec__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'copysign', 'cos', 'cosh', 'degrees', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'gcd', 'hypot', 'inf', 'isclose', 'isfinite','isinf', 'isnan', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'log2', 'modf', 'nan', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'tau', 'trunc'] ps:结果的是以列表的形式呈现出来的","text":"如何查看math标准库中的相关方法: 12import mathprint(dir(math)) 12345['__doc__', '__loader__', '__name__', '__package__', '__spec__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'copysign', 'cos', 'cosh', 'degrees', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'gcd', 'hypot', 'inf', 'isclose', 'isfinite','isinf', 'isnan', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'log2', 'modf', 'nan', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'tau', 'trunc'] ps:结果的是以列表的形式呈现出来的 库中方法分类(python3.6) 类别 个数 详情 三角函数类 13 cos(),cosh(),acos()acosh(),sin(),sinh(),asin(),asinh(),tan(),tanh(),atan(),atanh(),atan2() 常数类 3 pi,e,tau(2*pi) 常非数类 2 inf&lt;无穷&gt;,nan&lt;不是数&gt; 取整 3+1 ceil(),floor(),round()&lt;round()为python内置函数直接调用即可&gt;trunc()&lt;将小数部分直接砍掉&gt;–&gt;trunc(2.58585458678465)==2 is&lt;…&gt;判断函数 4 isclose(),isinf(),ifnan(),isfinite() 常规计算操作 7 求和:fsum();开方:sqrt();最小公约数:gcd();阶乘:factorial()模运算:fmod(x,y),modf()&lt;modf()展现小数部分&gt;绝对值:fabs(),abs() &lt;abs()为python内置函数直接调用&gt; 幂运算 6 exp(),frexp(),frex(),expm1(),pow(),ldexp(x,i)==x(2*i) 对数 4 log(),log10(),loglp(),log2() 魔法方法 5 &#39;__doc__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39; 判断函数正确性 2 erf(),erfc() 复杂数学公式实现 5 gamma函数:gamma(),lgamma()计算三角形斜边函数:hypot()弧度,角度之间转换:degrees()&lt;弧-&gt;角&gt;,radians()&lt;角-&gt;弧&gt; 方法使用过程中的注意事项1.相同功能的函数其准确性的比较: exp(2) &gt; e**2 ; log10(x) &gt; log(x,10)2.相同功能函数其执行速度比较: abs(x) &gt; fabs(x) (x必须为数值型)3.python-math库 学习手册(官方资料) ps:整理过程中有不恰当的地方可以在下方留言(＾Ｕ＾)ノ~ＹＯ ?","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-03-09T04:42:24.128Z","updated":"2020-03-09T14:17:51.086Z","comments":true,"path":"2020/03/09/hello-world/","link":"","permalink":"http://yoursite.com/2020/03/09/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}